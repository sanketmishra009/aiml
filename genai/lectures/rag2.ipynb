{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78627832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m137.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m385.2/385.2 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.5/17.5 MB\u001b[0m \u001b[31m104.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m116.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m340.3/340.3 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m96.8/96.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m330.6/330.6 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m142.0/142.0 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
      "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-exporter-otlp-proto-common==1.38.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-proto==1.38.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-sdk~=1.38.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
      "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain langchain-community langchain-huggingface chromadb ragatouille chonkie transformers accelerate bitsandbytes sentencepiece safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7c0e1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb 12 16:35:21 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.82.07              Driver Version: 580.82.07      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   36C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "284f8d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05713c2269c3438d87e80a0a5c55a4fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcf865c6fea24f1f93c3ed4c0135653f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "huggingface_doc.csv:   0%|          | 0.00/22.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cadfe1c704734caf8748033ade4f4355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2647 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2647\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset:\n",
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"m-ric/huggingface_doc\")\n",
    "print(f\"Number of documents: {len(data['train'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffce5a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text ,  Create an Endpoint\n",
      "\n",
      "After your first login, you will be directed to the [Endpoint creation page](https://ui.endpoints.huggingface.co/new). As an example, this guide will go through the steps to deploy [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) for text classification. \n",
      "\n",
      "## 1. Enter the Hugging Face Repository ID and your desired endpoint name:\n",
      "\n",
      "<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-docu...\n",
      "source. huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/create_endpoint.mdx\n"
     ]
    }
   ],
   "source": [
    "# print sample document:\n",
    "doc = data[\"train\"][0]\n",
    "\n",
    "for key, val in doc.items():\n",
    "    if isinstance(val, str) and len(val) > 500:\n",
    "        print(f\"{key} , {val[:500]}...\")\n",
    "    else:\n",
    "        print(f\"{key}. {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86523e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0a1c9be09d942c89a38cf1f3d1ead63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of chunk-lists: 2647\n"
     ]
    }
   ],
   "source": [
    "# convert text to chunks:\n",
    "from chonkie import TokenChunker\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_pretrained(\"gpt2\")\n",
    "chunker = TokenChunker(tokenizer=tokenizer, chunk_size=256, chunk_overlap=32)\n",
    "\n",
    "chunks = []\n",
    "for doc in data[\"train\"]:\n",
    "    chunk = chunker.chunk(doc[\"text\"])\n",
    "    chunks.append(chunk)\n",
    "\n",
    "print(f\"Total # of chunk-lists: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba748827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of chunks to embed:2647\n"
     ]
    }
   ],
   "source": [
    "ctexts = [chunk[0].text for chunk in chunks]\n",
    "print(f\"# of chunks to embed:{len(ctexts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d61c09e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d2c2a4ab88945da844849441500c23c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a222b93f024f3b9da8326e2c699a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71723910712f40cbba4ddc649063e003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cdde439885b46c0a55fad49047b7e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f42b0e7417044fe8c6c53cb5f7dd476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4a6cc5672141558aa6b7640e7538a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c6904f77d0498e9a90526924a4a693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4599b23a113d4f5a8e2efd3513c97d78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee8162db8164d1ca917bfafcffd9cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fde3ef5c7bc94cdb9f4497165e5ebe25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16acb9556e3c48cc8fd44c337e547e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded!(bi-encoder that processes queries and documents together!)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model=\"BAAI/bge-base-en-v1.5\")\n",
    "print(\n",
    "    \"Embedding model loaded!(bi-encoder that processes queries and documents together!)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05e647c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 2647 documents for indexing.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = []\n",
    "for i, text in enumerate(ctexts):\n",
    "    documents.append(\n",
    "        Document(\n",
    "            page_content=text, metadata={\"chunk_id\": i, \"source\": \"huggingface_doc\"}\n",
    "        )\n",
    "    )\n",
    "\n",
    "docs = filter_complex_metadata(documents)\n",
    "print(f\"Prepared {len(docs)} documents for indexing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "662e3f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created with HNSW indexing\n",
      "Total documents indexed: 2647\n"
     ]
    }
   ],
   "source": [
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"huggingface_docs_advanced\",\n",
    ")\n",
    "\n",
    "print(\"Vector store created with HNSW indexing\")\n",
    "print(f\"Total documents indexed: {vectorstore._collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e727846c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever configured:\n",
      "  - Search type: similarity (cosine distance)\n",
      "  - Initial candidates: 20\n",
      "  - Algorithm: HNSW (logarithmic search time)\n"
     ]
    }
   ],
   "source": [
    "# create retriver - bi-encoder for fast retrival!\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 20})\n",
    "\n",
    "print(\"Retriever configured:\")\n",
    "print(\"  - Search type: similarity (cosine distance)\")\n",
    "print(\"  - Initial candidates: 20\")\n",
    "print(\"  - Algorithm: HNSW (logarithmic search time)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cf89d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'How do I load a pretrained model?'\n",
      "Search time with HNSW: 83.48 ms\n",
      "Documents retrieved: 20\n",
      "\n",
      "================================================================================\n",
      "Top 3 Results:\n",
      "================================================================================\n",
      "\n",
      "Result 1:\n",
      " Quickstart\n",
      "\n",
      "This quickstart is intended for developers who are ready to dive into the code and see an example of how to integrate `timm` into their model training workflow.\n",
      "\n",
      "First, you'll need to ins...\n",
      "Source: huggingface_doc, Chunk ID: 1474\n",
      "\n",
      "Result 2:\n",
      "FrameworkSwitchCourse {fw} />\n",
      "\n",
      "# Fine-tuning a model with the Trainer API[[fine-tuning-a-model-with-the-trainer-api]]\n",
      "\n",
      "<CourseFloatingBanner chapter={3}\n",
      "  classNames=\"absolute z-10 right-0 top-0\"\n",
      "  no...\n",
      "Source: huggingface_doc, Chunk ID: 2398\n",
      "\n",
      "Result 3:\n",
      "FrameworkSwitchCourse {fw} />\n",
      "\n",
      "# Introduction[[introduction]]\n",
      "\n",
      "<CourseFloatingBanner\n",
      "    chapter={3}\n",
      "    classNames=\"absolute z-10 right-0 top-0\"\n",
      "/>\n",
      "\n",
      "In [Chapter 2](/course/chapter2) we explored how t...\n",
      "Source: huggingface_doc, Chunk ID: 2215\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Test query\n",
    "test_query = \"How do I load a pretrained model?\"\n",
    "\n",
    "# Measure search time with HNSW\n",
    "start_time = time.time()\n",
    "results = retriever.invoke(test_query)\n",
    "search_time = time.time() - start_time\n",
    "\n",
    "print(f\"Query: '{test_query}'\")\n",
    "print(f\"Search time with HNSW: {search_time*1000:.2f} ms\")\n",
    "print(f\"Documents retrieved: {len(results)}\\n\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Top 3 Results:\")\n",
    "print(\"=\" * 80)\n",
    "for i, doc in enumerate(results[:3], 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"{doc.page_content[:200]}...\")\n",
    "    print(f\"Source: {doc.metadata['source']}, Chunk ID: {doc.metadata['chunk_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e81e35",
   "metadata": {},
   "source": [
    "## HYDE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44b900af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "225cf307775a407dbbaed842d31cd754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c60c38dabbc3454eb038f3d9217c5c01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0cc18b9858f46589bd53b057138cf29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b21e3aeeeff45bf92fcccef1bc8137b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a6ee0464be4c4b97dca0addaf7b8e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd63976cc704545af0eabc5fd5b704a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d5f6dc447304989bfc27c9005bb0188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bdd3de26112461893989ac23b6b9402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd5b15c1cf043fd9c1a19bb488f899f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8386b8e4d9a0423fad5411c7a0aa69ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f3a7bb83ea1402ca6dba2461af8ec28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7396066a0c1d42e5b89a49e2d3788b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "754d5cf9c6bf4c1fb0231f7288e89389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 3584)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load qwen2.5-7b-instruct:\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "mid = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(mid, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    mid, quantization_config=bnb_config, device_map=\"auto\", trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ab82036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "untokenized chat template:\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Write a detailed, factual answer.\n",
      "Do not mention uncertainty.\n",
      "\n",
      "Question:\n",
      "What is vector similarity search in information retrieval?\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "tokenized chat template:\n",
      "tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
      "            553,  54364,  14817,     13,   1446,    525,    264,  10950,  17847,\n",
      "             13, 151645,    198, 151644,    872,    198,   7985,    264,  11682,\n",
      "             11,  59901,   4226,    624,   5404,    537,   6286,  26826,    382,\n",
      "          14582,    510,   3838,    374,   4621,  37623,   2711,    304,   1995,\n",
      "          56370,   5267, 151645,    198, 151644,  77091,    198]],\n",
      "       device='cuda:0')\n",
      "Generated text:system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
      "user\n",
      "Write a detailed, factual answer.\n",
      "Do not mention uncertainty.\n",
      "\n",
      "Question:\n",
      "What is vector similarity search in information retrieval?\n",
      "\n",
      "assistant\n",
      "Vector similarity search in information retrieval is a technique used to find items in a dataset that are similar to a query item, where both the items and the query are represented as vectors in a multi-dimensional space. This method is widely used in various applications such as recommendation systems, image retrieval, and natural language processing.\n",
      "\n",
      "In this context, a vector represents an item or document, with each dimension corresponding to a feature or attribute of the item. For example, in a recommendation system, user preferences can be represented as vectors, and items (like movies or products) can also be represented as vectors based on their features. The similarity between two vectors can be quantified using distance metrics such as Euclidean distance, cosine similarity, or other measures depending on the specific application.\n",
      "\n",
      "The process of vector similarity search involves the following steps:\n",
      "\n",
      "1. **Representation**: Items in the dataset are converted into numerical vectors. This can be done through techniques like word embeddings for text data, or feature extraction methods for non-textual data.\n",
      "\n",
      "2. **Query Representation**: A query is also converted into a vector form, representing the user's preferences or the characteristics they are searching for.\n",
      "\n",
      "3. **Similarity Calculation**: The similarity between the query vector and each item vector in the dataset is calculated. Commonly used similarity measures include cosine similarity, which measures the cosine of the angle between two vectors, and Euclidean distance, which measures the straight-line distance between two points in n-dimensional space.\n",
      "\n",
      "4. **Search and Ranking**: Items\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Write a detailed, factual answer.\n",
    "Do not mention uncertainty.\n",
    "\n",
    "Question:\n",
    "What is vector similarity search in information retrieval?\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False\n",
    ")\n",
    "print(f\"untokenized chat template:\\n{inputs}\")\n",
    "inputs = tokenizer(inputs, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "print(f\"tokenized chat template:\\n{inputs}\")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs, max_new_tokens=300, temperature=0.2, top_p=0.9, do_sample=True\n",
    "    )\n",
    "print(f\"Generated text:{tokenizer.decode(outputs[0] , skip_special_tokens= True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0eb753d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result 0:\n",
      "ext embeddings and semantic search. In this video weâ€™ll explore how Transformer models represent text as embedding vectors and how these vectors can be used to find similar documents in a corpus. Text...\n",
      "Source: huggingface_doc, Chunk ID: 1314\n",
      "\n",
      "Result 1:\n",
      "--\n",
      "title: Image Similarity with Hugging Face Datasets and Transformers\n",
      "thumbnail: /blog/assets/image_similarity/thumbnail.png\n",
      "authors:\n",
      "- user: sayakpaul\n",
      "---\n",
      "\n",
      "# Image Similarity with Hugging Face Datas...\n",
      "Source: huggingface_doc, Chunk ID: 2226\n",
      "\n",
      "Result 2:\n",
      " Search index\n",
      "\n",
      "[FAISS](https://github.com/facebookresearch/faiss) and [Elasticsearch](https://www.elastic.co/elasticsearch/) enables searching for examples in a dataset. This can be useful when you wa...\n",
      "Source: huggingface_doc, Chunk ID: 1172\n",
      "\n",
      "Result 3:\n",
      "--\n",
      "title: \"Retrieval Augmented Generation with Huggingface Transformers and Ray\"\n",
      "thumbnail: /blog/assets/12_ray_rag/ray_arch_updated.png\n",
      "authors:\n",
      "- user: ray-project\n",
      "  guest: true\n",
      "---\n",
      "\n",
      "# Retrieval Aug...\n",
      "Source: huggingface_doc, Chunk ID: 305\n",
      "\n",
      "Result 4:\n",
      "--\n",
      "title: \"Image search with ðŸ¤— datasets\"\n",
      "thumbnail: /blog/assets/54_image_search_datasets/spaces_image_search.jpg \n",
      "authors:\n",
      "- user: davanstrien\n",
      "  guest: true\n",
      "---\n",
      "\n",
      "# Image search with ðŸ¤— datasets  \n",
      "\n",
      "\n",
      "<a...\n",
      "Source: huggingface_doc, Chunk ID: 414\n",
      "\n",
      "Result 5:\n",
      "--\n",
      "title: 'Getting Started With Embeddings'\n",
      "thumbnail: /blog/assets/80_getting_started_with_embeddings/thumbnail.png\n",
      "authors:\n",
      "- user: espejelomar\n",
      "---\n",
      "\n",
      "# Getting Started With Embeddings\n",
      "\n",
      "\n",
      "Check out thi...\n",
      "Source: huggingface_doc, Chunk ID: 547\n",
      "\n",
      "Result 6:\n",
      "--\n",
      "title: Guiding Text Generation with Constrained Beam Search in ðŸ¤— Transformers\n",
      "thumbnail: /blog/assets/53_constrained_beam_search/thumbnail.png\n",
      "authors:\n",
      "- user: cwkeam\n",
      "  guest: true\n",
      "---\n",
      "\n",
      "# Guiding T...\n",
      "Source: huggingface_doc, Chunk ID: 160\n",
      "\n",
      "Result 7:\n",
      " Datasets ðŸ¤ Arrow\n",
      "\n",
      "## What is Arrow?\n",
      "\n",
      "[Arrow](https://arrow.apache.org/) enables large amounts of data to be processed and moved quickly. It is a specific data format that stores data in a columnar me...\n",
      "Source: huggingface_doc, Chunk ID: 2024\n",
      "\n",
      "Result 8:\n",
      "--\n",
      "title: \"Supercharged Searching on the ðŸ¤— Hub\"\n",
      "thumbnail: /blog/assets/48_hubsearch/thumbnail.png\n",
      "authors:\n",
      "- user: muellerzr\n",
      "---\n",
      "\n",
      "# Supercharged Searching on the Hugging Face Hub\n",
      "\n",
      "\n",
      "<a target=\"_blank\"...\n",
      "Source: huggingface_doc, Chunk ID: 1617\n",
      "\n",
      "Result 9:\n",
      "--\n",
      "title: \"Generating Human-level Text with Contrastive Search in Transformers ðŸ¤—\"\n",
      "thumbnail: /blog/assets/115_introducing_contrastive_search/thumbnail.png\n",
      "authors:\n",
      "- user: GMFTBY\n",
      "---\n",
      "\n",
      "# Generating Hum...\n",
      "Source: huggingface_doc, Chunk ID: 91\n",
      "\n",
      "Result 10:\n",
      " Search text in a dataset\n",
      "\n",
      "Datasets Server provides a `/search` endpoint for searching words in a dataset.\n",
      "\n",
      "<Tip warning={true}>\n",
      "  Currently, only <a href=\"./parquet\">datasets with Parquet exports</a>...\n",
      "Source: huggingface_doc, Chunk ID: 1690\n",
      "\n",
      "Result 11:\n",
      " Using Sentence Transformers at Hugging Face\n",
      "\n",
      "`sentence-transformers` is a library that provides easy methods to compute embeddings (dense vector representations) for sentences, paragraphs and images....\n",
      "Source: huggingface_doc, Chunk ID: 2046\n",
      "\n",
      "Result 12:\n",
      "--\n",
      "title: \"Train a Sentence Embedding Model with 1B Training Pairs\"\n",
      "authors:\n",
      "- user: asi\n",
      "  guest: true\n",
      "---\n",
      "\n",
      "# Train a Sentence Embedding Model with 1 Billion Training Pairs\n",
      "\n",
      "\n",
      "**Sentence embedding** is...\n",
      "Source: huggingface_doc, Chunk ID: 518\n",
      "\n",
      "Result 13:\n",
      "--\n",
      "title: \"Accelerating Document AI\" \n",
      "thumbnail: /blog/assets/112_document-ai/thumbnail.png\n",
      "authors:\n",
      "- user: rajistics\n",
      "- user: nielsr\n",
      "- user: florentgbelidji\n",
      "- user: nbroad\n",
      "---\n",
      "\n",
      "# Accelerating Documen...\n",
      "Source: huggingface_doc, Chunk ID: 144\n",
      "\n",
      "Result 14:\n",
      " Search\n",
      "\n",
      "You can now easily search anything on the Hub with **Full-text search**. We index model cards, dataset cards, and Spaces app.py files.\n",
      "\n",
      "Go directly to https://huggingface.co/search or, using ...\n",
      "Source: huggingface_doc, Chunk ID: 384\n",
      "\n",
      "Result 15:\n",
      "--\n",
      "title: 'Few-shot learning in practice: GPT-Neo and the ðŸ¤— Accelerated Inference API'\n",
      "# thumbnail: /blog/assets/22_few_shot_learning_gpt_neo_and_inference_api/thumbnail.png\n",
      "authors:\n",
      "- user: philschmi...\n",
      "Source: huggingface_doc, Chunk ID: 2541\n",
      "\n",
      "Result 16:\n",
      "--\n",
      "title:  Deploy Embedding Models with Hugging Face Inference Endpoints\n",
      "thumbnail: /blog/assets/168_inference_endpoints_embeddings/thumbnail.jpg\n",
      "authors:\n",
      "- user: philschmid\n",
      "---\n",
      "\n",
      "# Deploy Embedding Mo...\n",
      "Source: huggingface_doc, Chunk ID: 2464\n",
      "\n",
      "Result 17:\n",
      "--\n",
      "title: \"Introduction to Graph Machine Learning\" \n",
      "thumbnail: /blog/assets/125_intro-to-graphml/thumbnail.png\n",
      "authors:\n",
      "- user: clefourrier\n",
      "---\n",
      "\n",
      "# Introduction to Graph Machine Learning\n",
      "\n",
      "\n",
      "In this blog...\n",
      "Source: huggingface_doc, Chunk ID: 401\n",
      "\n",
      "Result 18:\n",
      "emory mapping and streaming. In this video we'll take a look at two core features of the Datasets library that allow you to load and process huge datasets without blowing up your laptop's CPU. \n",
      "\n",
      "Nowad...\n",
      "Source: huggingface_doc, Chunk ID: 1854\n",
      "\n",
      "Result 19:\n",
      "--\n",
      "title: \"Introducing the Data Measurements Tool: an Interactive Tool for Looking at Datasets\"\n",
      "thumbnail: /blog/assets/37_data-measurements-tool/datametrics.png\n",
      "authors:\n",
      "- user: sasha\n",
      "- user: yjernit...\n",
      "Source: huggingface_doc, Chunk ID: 1825\n"
     ]
    }
   ],
   "source": [
    "## function for hyde search:\n",
    "def hyde_gen_doc(query, model, tokenizer):\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": query}],\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs, max_new_tokens=300, temperature=0.2, top_p=0.9, do_sample=True\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def hyde_search(query, model, tokenizer, retriever):\n",
    "    hdoc = hyde_gen_doc(query, model, tokenizer)\n",
    "    results = retriever.invoke(hdoc)\n",
    "    return results\n",
    "\n",
    "\n",
    "retrieved_docs = hyde_search(prompt, model, tokenizer, retriever)\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"{doc.page_content[:200]}...\")\n",
    "    print(f\"Source: {doc.metadata['source']}, Chunk ID: {doc.metadata['chunk_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aaf4ff57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfef7ea799c6470a98ce833795a1f268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba6446a04cd4497696346a116ed9e7bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7c132906d1e42fdb20095cf83d879de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f3cbf9270c4b51bb4b082173ae1203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de7b0e10159248b79f39210c5ff20b07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54500879a3f841bead1a47051ba12112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ce6a51faadc4b4aa21513f4d57d56a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-encoder reranker loaded\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Load a cross-encoder model for reranking\n",
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\", device=\"cuda\")\n",
    "\n",
    "print(\"Cross-encoder reranker loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93baa47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_docs(query, documents, reranker, k=5):\n",
    "    # make pairs\n",
    "    pairs = [(query, doc) for doc in documents]\n",
    "\n",
    "    # get scores and merge with documents:\n",
    "    scores = reranker.predict(pairs)\n",
    "    rdocs = [(doc, score) for doc, score in zip(documents, scores)]\n",
    "\n",
    "    # rank documents according to scores and return:\n",
    "\n",
    "    print(f\"{rdocs}\")\n",
    "    rdocs.sort(key=lambda x: x[1], reverse=True)\n",
    "    # return top-k documents:\n",
    "    results = []\n",
    "    for doc, score in rdocs[:k]:\n",
    "        results.append({\"content\": doc, \"score\": float(score)})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7875c91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"ext embeddings and semantic search. In this video weâ€™ll explore how Transformer models represent text as embedding vectors and how these vectors can be used to find similar documents in a corpus. Text embeddings are just a fancy way of saying that we can represent text as an array of numbers called a vector. To create these embeddings we usually use an encoder-based model like BERT. In this example, you can see how we feed three sentences to the encoder and get three vectors as the output. Reading the text, we can see that walking the dog seems to be most similar to walking the cat, but let's see if we can quantify this! The trick to do the comparison is to compute a similarity metric between each pair of embedding vectors. These vectors usually live in a high-dimensional space, so a similarity metric can be anything that measures some sort of distance between vectors. One popular metric is cosine similarity, which uses the angle between two vectors to measure how close they are. In this example, our embedding vectors live in 3D and we can see that the orange and grey vectors are close to each other and have a smaller angle. Now one problem we have to deal with is that Transformer models like\", np.float32(-5.5047407)), ('--\\ntitle: Image Similarity with Hugging Face Datasets and Transformers\\nthumbnail: /blog/assets/image_similarity/thumbnail.png\\nauthors:\\n- user: sayakpaul\\n---\\n\\n# Image Similarity with Hugging Face Datasets and Transformers\\n\\n\\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_similarity.ipynb\">\\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\\n</a>\\n\\nIn this post, you\\'ll learn to build an image similarity system with ðŸ¤— Transformers. Finding out the similarity between a query image and potential candidates is an important use case for information retrieval systems, such as reverse image search, for example. All the system is trying to answer is that, given a _query_ image and a set of _candidate_ images, which images are the most similar to the query image. \\n\\nWe\\'ll leverage the [ðŸ¤— `datasets`', np.float32(-8.19003)), (' Search index\\n\\n[FAISS](https://github.com/facebookresearch/faiss) and [Elasticsearch](https://www.elastic.co/elasticsearch/) enables searching for examples in a dataset. This can be useful when you want to retrieve specific examples from a dataset that are relevant to your NLP task. For example, if you are working on a Open Domain Question Answering task, you may want to only return examples that are relevant to answering your question.\\n\\nThis guide will show you how to build an index for your dataset that will allow you to search it.\\n\\n## FAISS\\n\\nFAISS retrieves documents based on the similarity of their vector representations. In this example, you will generate the vector representations with the [DPR](https://huggingface.co/transformers/model_doc/dpr.html) model.\\n\\n1. Download the DPR model from ðŸ¤— Transformers:\\n\\n```py\\n>>> from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\\n>>> import torch\\n>>> torch.set_grad_enabled(False)\\n>>> ctx_encoder = DPRContextEncoder.from_pretrained(\"facebook', np.float32(-4.1617618)), ('--\\ntitle: \"Retrieval Augmented Generation with Huggingface Transformers and Ray\"\\nthumbnail: /blog/assets/12_ray_rag/ray_arch_updated.png\\nauthors:\\n- user: ray-project\\n  guest: true\\n---\\n\\n# Retrieval Augmented Generation with Huggingface Transformers and Ray\\n\\n\\n##### A guest blog post by <a href=\"/amogkam\">Amog Kamsetty</a> from the Anyscale team\\n\\n[Huggingface Transformers](https://huggingface.co/) recently added the [Retrieval Augmented Generation (RAG)](https://twitter.com/huggingface/status/1310597560906780680) model, a new NLP architecture that leverages external documents (like Wikipedia) to augment its knowledge and achieve state of the art results on knowledge-intensive tasks. In this blog post, we introduce the integration of [Ray](https://docs.ray.io/en/master/), a library for building scalable applications, into the RAG contextual document retrieval mechanism. This speeds up retrieval calls by 2x and improves the scalability of RAG distributed [fine-tun', np.float32(-10.702507)), ('--\\ntitle: \"Image search with ðŸ¤— datasets\"\\nthumbnail: /blog/assets/54_image_search_datasets/spaces_image_search.jpg \\nauthors:\\n- user: davanstrien\\n  guest: true\\n---\\n\\n# Image search with ðŸ¤— datasets  \\n\\n\\n<a target=\"_blank\" href=\"https://colab.research.google.com/gist/davanstrien/e2c29fbbed20dc767e5a74e210f4237b/hf_blog_image_search.ipynb\">\\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\\n</a>\\n\\nðŸ¤— [`datasets`](https://huggingface.co/docs/datasets/) is a library that makes it easy to access and share datasets. It also makes it easy to process data efficiently -- including working with data which doesn\\'t fit into memory.\\n\\nWhen `datasets` was first launched, it was associated mostly with text data.', np.float32(-10.953519)), ('--\\ntitle: \\'Getting Started With Embeddings\\'\\nthumbnail: /blog/assets/80_getting_started_with_embeddings/thumbnail.png\\nauthors:\\n- user: espejelomar\\n---\\n\\n# Getting Started With Embeddings\\n\\n\\nCheck out this tutorial with the Notebook Companion:\\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/80_getting_started_with_embeddings.ipynb\">\\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\\n</a>\\n\\n## Understanding embeddings\\n\\nAn embedding is a numerical representation of a piece of information, for example, text, documents, images, audio, etc. The representation captures the semantic meaning of what is being embedded, making it robust for many industry applications.\\n\\nGiven the text \"What is the main benefit of voting?\", an embedding of the sentence could be represented in a vector space, for example, with a', np.float32(-11.180451)), ('--\\ntitle: Guiding Text Generation with Constrained Beam Search in ðŸ¤— Transformers\\nthumbnail: /blog/assets/53_constrained_beam_search/thumbnail.png\\nauthors:\\n- user: cwkeam\\n  guest: true\\n---\\n\\n# Guiding Text Generation with Constrained Beam Search in ðŸ¤— Transformers\\n\\n\\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/53_constrained_beam_search.ipynb\">\\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\\n</a>\\n\\n\\n## **Introduction**\\n\\nThis blog post assumes that the reader is familiar with text generation methods using the different variants of beam search, as explained in the blog post: [\"How to generate text: using different decoding methods for language generation with Transformers\"](https://huggingface.co/blog/how-to-generate)\\n\\nUnlike ordinary beam search, **constrained** beam', np.float32(-11.094301)), (\" Datasets ðŸ¤ Arrow\\n\\n## What is Arrow?\\n\\n[Arrow](https://arrow.apache.org/) enables large amounts of data to be processed and moved quickly. It is a specific data format that stores data in a columnar memory layout. This provides several significant advantages:\\n\\n* Arrow's standard format allows [zero-copy reads](https://en.wikipedia.org/wiki/Zero-copy) which removes virtually all serialization overhead.\\n* Arrow is language-agnostic so it supports different programming languages.\\n* Arrow is column-oriented so it is faster at querying and processing slices or columns of data.\\n* Arrow allows for copy-free hand-offs to standard machine learning tools such as NumPy, Pandas, PyTorch, and TensorFlow.\\n* Arrow supports many, possibly nested, column types.\\n\\n## Memory-mapping\\n\\nðŸ¤— Datasets uses Arrow for its local caching system. It allows datasets to be backed by an on-disk cache, which is memory-mapped for fast lookup.\\nThis architecture allows for large datasets to be used on machines with relatively small device memory.\\n\\nFor example, loading the full English Wikipedia dataset only\", np.float32(-10.995365)), ('--\\ntitle: \"Supercharged Searching on the ðŸ¤— Hub\"\\nthumbnail: /blog/assets/48_hubsearch/thumbnail.png\\nauthors:\\n- user: muellerzr\\n---\\n\\n# Supercharged Searching on the Hugging Face Hub\\n\\n\\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/muellerzr/hf-blog-notebooks/blob/main/Searching-the-Hub.ipynb\">\\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\\n</a>\\n\\nThe `huggingface_hub` library is a lightweight interface that provides a programmatic approach to exploring the hosting endpoints Hugging Face provides: models, datasets, and Spaces.\\n\\nUp until now, searching on the Hub through this interface was tricky to pull off, and there were many aspects of it a user had to \"just know\" and get accustomed to. \\n\\nIn this article, we will be looking at a few exciting new features added to `hugging', np.float32(-11.170016)), ('--\\ntitle: \"Generating Human-level Text with Contrastive Search in Transformers ðŸ¤—\"\\nthumbnail: /blog/assets/115_introducing_contrastive_search/thumbnail.png\\nauthors:\\n- user: GMFTBY\\n---\\n\\n# Generating Human-level Text with Contrastive Search in Transformers ðŸ¤—\\n\\n\\n****\\n\\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/115_introducing_contrastive_search.ipynb\">\\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\\n</a>\\n\\n### 1. Introduction:\\n\\nNatural language generation (i.e. text generation) is one of the core tasks in natural language processing (NLP). In this blog, we introduce the current state-of-the-art decoding method, ___Contrastive Search___, for neural text generation. Contrastive search is originally proposed in _\"A Contrastive Framework for Neural Text Generation\"_', np.float32(-11.029544)), (' Search text in a dataset\\n\\nDatasets Server provides a `/search` endpoint for searching words in a dataset.\\n\\n<Tip warning={true}>\\n  Currently, only <a href=\"./parquet\">datasets with Parquet exports</a>\\n  are supported so Datasets Server can index the contents and run the search without\\n  downloading the whole dataset.\\n</Tip>\\n\\nThis guide shows you how to use Datasets Server\\'s `/search` endpoint to search for a query string.\\nFeel free to also try it out with [ReDoc](https://redocly.github.io/redoc/?url=https://datasets-server.huggingface.co/openapi.json#operation/searchRows).\\n\\nThe text is searched in the columns of type `string`, even if the values are nested in a dictionary.\\n\\nThe `/search` endpoint accepts five query parameters:\\n\\n- `dataset`: the dataset name, for example `glue` or `mozilla-foundation/common_voice_10_0`\\n- `config`: the configuration name, for example `cola`\\n-', np.float32(-10.547512)), (' Using Sentence Transformers at Hugging Face\\n\\n`sentence-transformers` is a library that provides easy methods to compute embeddings (dense vector representations) for sentences, paragraphs and images. Texts are embedded in a vector space such that similar text is close, which enables applications such as semantic search, clustering, and retrieval. \\n\\n## Exploring sentence-transformers in the Hub\\n\\nYou can find over 500 hundred `sentence-transformer` models by filtering at the left of the [models page](https://huggingface.co/models?library=sentence-transformers&sort=downloads). Most of these models support different tasks, such as doing [`feature-extraction`](https://huggingface.co/models?library=sentence-transformers&pipeline_tag=feature-extraction&sort=downloads) to generate the embedding, and [`sentence-similarity`](https://huggingface.co/models?library=sentence-transformers&pipeline_tag=sentence-similarity&sort=downloads) as a way to determine how similar is a given sentence to other. You', np.float32(-7.343368)), ('--\\ntitle: \"Train a Sentence Embedding Model with 1B Training Pairs\"\\nauthors:\\n- user: asi\\n  guest: true\\n---\\n\\n# Train a Sentence Embedding Model with 1 Billion Training Pairs\\n\\n\\n**Sentence embedding** is a method that maps sentences to vectors of real numbers. Ideally, these vectors would capture the semantic of a sentence and be highly generic. Such representations could then be used for many downstream applications such as clustering, text mining, or question answering.\\n\\nWe developed state-of-the-art sentence embedding models as part of the project [\"Train the Best Sentence Embedding Model Ever with 1B Training Pairs\"](https://discuss.huggingface.co/t/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs/7354). This project took place during the [Community week using JAX/Flax for NLP & CV](https://discuss.huggingface.co/t/open-to-the-community-community-week-using-jax-flax-for-nlp-cv/7', np.float32(-10.435896)), ('--\\ntitle: \"Accelerating Document AI\" \\nthumbnail: /blog/assets/112_document-ai/thumbnail.png\\nauthors:\\n- user: rajistics\\n- user: nielsr\\n- user: florentgbelidji\\n- user: nbroad\\n---\\n\\n# Accelerating Document AI\\n\\n\\n\\nEnterprises are full of documents containing knowledge that isn\\'t accessible by digital workflows. These documents can vary from letters, invoices, forms, reports, to receipts. With the improvements in text, vision, and multimodal AI, it\\'s now possible to unlock that information. This post shows you how your teams can use open-source models to build custom solutions for free!\\n\\nDocument AI includes many data science tasks from [image classification](https://huggingface.co/tasks/image-classification), [image to text](https://huggingface.co/tasks/image-to-text), [document question answering](https://huggingface.co/tasks/document-question-answering), [table question answering](https://huggingface.co/tasks/table-question-ans', np.float32(-11.15997)), (' Search\\n\\nYou can now easily search anything on the Hub with **Full-text search**. We index model cards, dataset cards, and Spaces app.py files.\\n\\nGo directly to https://huggingface.co/search or, using the search bar at the top of https://huggingface.co, you can select \"Try Full-text search\" to help find what you seek on the Hub across models, datasets, and Spaces:\\n\\n<div class=\"flex justify-center\">\\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/fulltextsearch1.png\"/>\\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/fulltextsearch2.png\"/>\\n</div>\\n\\n<div class=\"flex justify-center\">\\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/res', np.float32(-11.143152)), (\"--\\ntitle: 'Few-shot learning in practice: GPT-Neo and the ðŸ¤— Accelerated Inference API'\\n# thumbnail: /blog/assets/22_few_shot_learning_gpt_neo_and_inference_api/thumbnail.png\\nauthors:\\n- user: philschmid\\n---\\n\\n# Few-shot learning in practice: GPT-Neo and the ðŸ¤— Accelerated Inference API\\n\\n\\nIn many Machine Learning applications, the amount of available labeled data is a barrier to producing a high-performing model. The latest developments in NLP show that you can overcome this limitation by providing a few examples at inference time with a large language model - a technique known as Few-Shot Learning. In this blog post, we'll explain what Few-Shot Learning is, and explore how a large language model called GPT-Neo, and the ðŸ¤— Accelerated Inference API, can be used to generate your own predictions.\\n\\n\\n## What is Few-Shot Learning?\\n\\nFew-Shot Learning refers to the practice of feeding a machine learning model with a very small amount of training data to guide its predictions, like a few examples at inference time\", np.float32(-11.279937)), ('--\\ntitle:  Deploy Embedding Models with Hugging Face Inference Endpoints\\nthumbnail: /blog/assets/168_inference_endpoints_embeddings/thumbnail.jpg\\nauthors:\\n- user: philschmid\\n---\\n\\n# Deploy Embedding Models with Hugging Face Inference Endpoints\\n\\nThe rise of Generative AI and LLMs like ChatGPT has increased the interest and importance of embedding models for a variety of tasks especially for retrievel augemented generation, like search or chat with your data. Embeddings are helpful since they represent sentences, images, words, etc. as numeric vector representations, which allows us to map semantically related items and retrieve helpful information. This helps us to provide relevant context for our prompt to improve the quality and specificity of generation. \\n\\nCompared to LLMs are Embedding Models smaller in size and faster for inference. That is very important since you need to recreate your embeddings after you changed your model or improved your model fine-tuning. Additionally, is it important that the whole retrieval augmentation process is as fast as possible to provide a good user experience. \\n\\nIn this blog post, we will show you', np.float32(-10.137062)), ('--\\ntitle: \"Introduction to Graph Machine Learning\" \\nthumbnail: /blog/assets/125_intro-to-graphml/thumbnail.png\\nauthors:\\n- user: clefourrier\\n---\\n\\n# Introduction to Graph Machine Learning\\n\\n\\nIn this blog post, we cover the basics of graph machine learning. \\n\\nWe first study what graphs are, why they are used, and how best to represent them. We then cover briefly how people learn on graphs, from pre-neural methods (exploring graph features at the same time) to what are commonly called Graph Neural Networks. Lastly, we peek into the world of Transformers for graphs.\\n\\n## Graphs\\n\\n### What is a graph?\\n\\nIn its essence, a graph is a description of items linked by relations.\\n\\nExamples of graphs include social networks (Twitter, Mastodon, any citation networks linking papers and authors), molecules, knowledge graphs (such as UML diagrams, encyclopedias, and any website with hyperlinks between its pages), sentences expressed as their syntactic trees, any 3D mesh, and more! It is, therefore, not hyperbolic to say that graphs are everywhere.\\n\\nThe items of a', np.float32(-11.194386)), (\"emory mapping and streaming. In this video we'll take a look at two core features of the Datasets library that allow you to load and process huge datasets without blowing up your laptop's CPU. \\n\\nNowadays it is not uncommon to find yourself working with multi-GB sized datasets, especially if youâ€™re planning to pretrain a transformer like BERT or GPT-2 from scratch. In these cases, even *loading* the data can be a challenge. For example, the C4 corpus used to\\npretrain T5 consists of over 2 terabytes of data!\\n\\nTo handle these large datasets, the Datasets library is built on two core features: the Apache Arrow format and a streaming API. Arrow is designed for high-performance data processing and represents each table-like dataset with an in-memory columnar format. As you can see in this example, columnar formats group the elements of a table in consecutive blocks of RAM and this unlocks fast access and processing. Arrow is great at processing data at any scale, but some datasets are so large that you can't even fit them on your hard disk. For these cases, the Datasets library provides a streaming API that allows you to progressively download the raw data\", np.float32(-11.301039)), ('--\\ntitle: \"Introducing the Data Measurements Tool: an Interactive Tool for Looking at Datasets\"\\nthumbnail: /blog/assets/37_data-measurements-tool/datametrics.png\\nauthors:\\n- user: sasha\\n- user: yjernite\\n- user: meg\\n---\\n\\n# Introducing the ðŸ¤— Data Measurements Tool: an Interactive Tool for Looking at Datasets\\n\\n\\n\\n\\n***tl;dr:*** We made a tool you can use online to build, measure, and compare datasets.\\n\\n[Click to access the ðŸ¤— Data Measurements Tool here.](https://huggingface.co/spaces/huggingface/data-measurements-tool)\\n\\n-----\\n\\nAs developers of a fast-growing unified repository for Machine Learning datasets ([Lhoest et al. 2021](https://arxiv.org/abs/2109.02846)), the ðŸ¤— Hugging Face [team](https://huggingface.co/huggingface) has been working on supporting good practices for dataset documentation ([McMillan-Major et al., 2021](https://arxiv', np.float32(-11.229602))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'content': ' Search index\\n\\n[FAISS](https://github.com/facebookresearch/faiss) and [Elasticsearch](https://www.elastic.co/elasticsearch/) enables searching for examples in a dataset. This can be useful when you want to retrieve specific examples from a dataset that are relevant to your NLP task. For example, if you are working on a Open Domain Question Answering task, you may want to only return examples that are relevant to answering your question.\\n\\nThis guide will show you how to build an index for your dataset that will allow you to search it.\\n\\n## FAISS\\n\\nFAISS retrieves documents based on the similarity of their vector representations. In this example, you will generate the vector representations with the [DPR](https://huggingface.co/transformers/model_doc/dpr.html) model.\\n\\n1. Download the DPR model from ðŸ¤— Transformers:\\n\\n```py\\n>>> from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\\n>>> import torch\\n>>> torch.set_grad_enabled(False)\\n>>> ctx_encoder = DPRContextEncoder.from_pretrained(\"facebook',\n",
       "  'score': -4.16176176071167},\n",
       " {'content': \"ext embeddings and semantic search. In this video weâ€™ll explore how Transformer models represent text as embedding vectors and how these vectors can be used to find similar documents in a corpus. Text embeddings are just a fancy way of saying that we can represent text as an array of numbers called a vector. To create these embeddings we usually use an encoder-based model like BERT. In this example, you can see how we feed three sentences to the encoder and get three vectors as the output. Reading the text, we can see that walking the dog seems to be most similar to walking the cat, but let's see if we can quantify this! The trick to do the comparison is to compute a similarity metric between each pair of embedding vectors. These vectors usually live in a high-dimensional space, so a similarity metric can be anything that measures some sort of distance between vectors. One popular metric is cosine similarity, which uses the angle between two vectors to measure how close they are. In this example, our embedding vectors live in 3D and we can see that the orange and grey vectors are close to each other and have a smaller angle. Now one problem we have to deal with is that Transformer models like\",\n",
       "  'score': -5.5047407150268555},\n",
       " {'content': ' Using Sentence Transformers at Hugging Face\\n\\n`sentence-transformers` is a library that provides easy methods to compute embeddings (dense vector representations) for sentences, paragraphs and images. Texts are embedded in a vector space such that similar text is close, which enables applications such as semantic search, clustering, and retrieval. \\n\\n## Exploring sentence-transformers in the Hub\\n\\nYou can find over 500 hundred `sentence-transformer` models by filtering at the left of the [models page](https://huggingface.co/models?library=sentence-transformers&sort=downloads). Most of these models support different tasks, such as doing [`feature-extraction`](https://huggingface.co/models?library=sentence-transformers&pipeline_tag=feature-extraction&sort=downloads) to generate the embedding, and [`sentence-similarity`](https://huggingface.co/models?library=sentence-transformers&pipeline_tag=sentence-similarity&sort=downloads) as a way to determine how similar is a given sentence to other. You',\n",
       "  'score': -7.343368053436279},\n",
       " {'content': '--\\ntitle: Image Similarity with Hugging Face Datasets and Transformers\\nthumbnail: /blog/assets/image_similarity/thumbnail.png\\nauthors:\\n- user: sayakpaul\\n---\\n\\n# Image Similarity with Hugging Face Datasets and Transformers\\n\\n\\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_similarity.ipynb\">\\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\\n</a>\\n\\nIn this post, you\\'ll learn to build an image similarity system with ðŸ¤— Transformers. Finding out the similarity between a query image and potential candidates is an important use case for information retrieval systems, such as reverse image search, for example. All the system is trying to answer is that, given a _query_ image and a set of _candidate_ images, which images are the most similar to the query image. \\n\\nWe\\'ll leverage the [ðŸ¤— `datasets`',\n",
       "  'score': -8.190030097961426},\n",
       " {'content': '--\\ntitle:  Deploy Embedding Models with Hugging Face Inference Endpoints\\nthumbnail: /blog/assets/168_inference_endpoints_embeddings/thumbnail.jpg\\nauthors:\\n- user: philschmid\\n---\\n\\n# Deploy Embedding Models with Hugging Face Inference Endpoints\\n\\nThe rise of Generative AI and LLMs like ChatGPT has increased the interest and importance of embedding models for a variety of tasks especially for retrievel augemented generation, like search or chat with your data. Embeddings are helpful since they represent sentences, images, words, etc. as numeric vector representations, which allows us to map semantically related items and retrieve helpful information. This helps us to provide relevant context for our prompt to improve the quality and specificity of generation. \\n\\nCompared to LLMs are Embedding Models smaller in size and faster for inference. That is very important since you need to recreate your embeddings after you changed your model or improved your model fine-tuning. Additionally, is it important that the whole retrieval augmentation process is as fast as possible to provide a good user experience. \\n\\nIn this blog post, we will show you',\n",
       "  'score': -10.137062072753906}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return cross ranked documents:\n",
    "bidocs = [doc.page_content for doc in retrieved_docs]\n",
    "rerank_docs(prompt, bidocs, reranker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d3e06e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
