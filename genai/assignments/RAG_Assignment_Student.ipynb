{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ms3vnJY_Mni"
      },
      "source": [
        "# RAG with HuggingFace and Milvus - Student Notebook\n",
        "\n",
        "In this assignment, you will implement a complete RAG (Retrieval-Augmented Generation) pipeline using:\n",
        "\n",
        "- **Dataset**: HuggingFace Documentation (`m-ric/huggingface_doc`)\n",
        "- **Vector Store**: Milvus\n",
        "- **Embeddings**: BGE-small-en-v1.5\n",
        "- **LLM**: Microsoft Phi-3-mini-4k-instruct/\"Qwen/Qwen2-1.5B-Instruct\"\n",
        "- **Evaluation**: Opik (AnswerRelevance, Hallucination)\n",
        "\n",
        "## Instructions\n",
        "\n",
        "1. Read through each section carefully\n",
        "2. Complete the code in cells marked with `# TODO`\n",
        "3. Run all cells in order\n",
        "4. Verify your implementation with the evaluation cells\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvdmsptpGbVw"
      },
      "source": [
        "##https://github.com/milvus-io/milvus\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBlF01io_Mnj"
      },
      "source": [
        "## 1. Setup\n",
        "\n",
        "Install required dependencies and configure environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "iq5jJcLv_Mnj"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q pymilvus sentence-transformers datasets transformers torch accelerate opik tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IEugGiLC_Mnk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment configured!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from typing import List, Dict, Tuple\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "print(\"Environment configured!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pHDkDSt_Mnk"
      },
      "source": [
        "## 2. Data Loading\n",
        "\n",
        "Load the HuggingFace documentation dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hfrrSuwy_Mnk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded with 2647 documents\n",
            "Columns: ['text', 'source']\n",
            "\n",
            "Sample document (first 500 chars):\n",
            " Create an Endpoint\n",
            "\n",
            "After your first login, you will be directed to the [Endpoint creation page](https://ui.endpoints.huggingface.co/new). As an example, this guide will go through the steps to deploy [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) for text classification. \n",
            "\n",
            "## 1. Enter the Hugging Face Repository ID and your desired endpoint name:\n",
            "\n",
            "<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-docu\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the HuggingFace documentation dataset\n",
        "dataset = load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\n",
        "\n",
        "print(f\"Dataset loaded with {len(dataset)} documents\")\n",
        "print(f\"Columns: {dataset.column_names}\")\n",
        "print(f\"\\nSample document (first 500 chars):\")\n",
        "print(dataset[0][\"text\"][:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "VOoX5-1-_Mnk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted 2647 documents\n",
            "Using 500 documents for this assignment\n"
          ]
        }
      ],
      "source": [
        "# Extract text and source information\n",
        "documents = []\n",
        "for item in dataset:\n",
        "    documents.append({\"text\": item[\"text\"], \"source\": item[\"source\"]})\n",
        "\n",
        "print(f\"Extracted {len(documents)} documents\")\n",
        "\n",
        "# For this assignment, we'll use a subset to keep things manageable\n",
        "MAX_DOCS = 500\n",
        "documents = documents[:MAX_DOCS]\n",
        "print(f\"Using {len(documents)} documents for this assignment\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgDpZKnL_Mnk"
      },
      "source": [
        "## 3. Chunking\n",
        "\n",
        "Split documents into smaller chunks for better retrieval.\n",
        "\n",
        "### Your Task\n",
        "\n",
        "Implement the `chunk_document` function that:\n",
        "\n",
        "1. Takes a text string, chunk_size, and chunk_overlap as parameters\n",
        "2. Splits the text into overlapping chunks of the specified size\n",
        "3. Returns a list of chunk strings\n",
        "\n",
        "### Hints\n",
        "\n",
        "- Use a sliding window approach with step = chunk_size - chunk_overlap\n",
        "- Handle edge cases: empty text, text shorter than chunk_size\n",
        "- Make sure each chunk is non-empty before adding it\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = [1, 2, 3, 4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WOXtgseg_Mnl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Created 3 chunks from 500 documents\n",
            "Average chunks per document: 0.01\n",
            "\n",
            "Sample chunk:\n",
            "  ID: 0\n",
            "  Source: huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/create_endpoint.mdx\n",
            "  Text (first 200 chars):  Create an Endpoint\n",
            "\n",
            "After your first login, you will be directed to the [Endpoint creation page](https://ui.endpoints.huggingface.co/new). As an example, this guide will go through the steps to deplo...\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# TODO: IMPLEMENT CHUNKING (15 points)\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "def chunk_document(\n",
        "    text: str, chunk_size: int = 1000, chunk_overlap: int = 200\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Split a document into overlapping chunks of fixed size.\n",
        "\n",
        "    Args:\n",
        "        text: The document text to chunk\n",
        "        chunk_size: Maximum size of each chunk in characters\n",
        "        chunk_overlap: Number of overlapping characters between chunks\n",
        "\n",
        "    Returns:\n",
        "        List of text chunks\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "\n",
        "    # TODO: Implement fixed-size chunking with overlap\n",
        "    #\n",
        "    # Step 1: Handle edge cases (empty text, short text)\n",
        "    # Hint: If text is empty or shorter than chunk_size, return it as a single chunk\n",
        "\n",
        "    if not text or len(text) < chunk_size:\n",
        "        return [text]\n",
        "\n",
        "    # Step 2: Calculate step size for sliding window\n",
        "    # Hint: step = chunk_size - chunk_overlap\n",
        "\n",
        "    step = chunk_size - chunk_overlap\n",
        "    # print(f\"{step=}\")\n",
        "\n",
        "    # Step 3: Create chunks using a while loop\n",
        "    # Hint: Start at position 0, extract chunk_size characters,\n",
        "    #       then move forward by step size\n",
        "\n",
        "    l = 0\n",
        "    r = chunk_size\n",
        "    while r < len(text):\n",
        "        # print(f\"{l=} , {r=}\")\n",
        "        chunk = text[l:r]\n",
        "        # check if next jump is full size or not:\n",
        "        l += step\n",
        "        r += step\n",
        "        # Step 4: Only add non-empty chunks (use .strip() to check)\n",
        "\n",
        "        if chunk.strip():\n",
        "            chunks.append(chunk)\n",
        "\n",
        "    # print(len(chunks))\n",
        "\n",
        "    if r + step > len(text):\n",
        "        l += step\n",
        "        r = len(text)\n",
        "        # print(f\"{l=} , {r=}\")\n",
        "        chunk = text[l:r]\n",
        "        chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def chunk_all_documents(\n",
        "    documents: List[Dict], chunk_size: int = 1000, chunk_overlap: int = 200\n",
        ") -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Chunk all documents and preserve metadata.\n",
        "\n",
        "    Args:\n",
        "        documents: List of document dicts with 'text' and 'source' keys\n",
        "        chunk_size: Maximum chunk size\n",
        "        chunk_overlap: Overlap between chunks\n",
        "\n",
        "    Returns:\n",
        "        List of chunk dicts with 'text', 'source', and 'chunk_id' keys\n",
        "    \"\"\"\n",
        "    all_chunks = []\n",
        "    chunk_id = 0\n",
        "\n",
        "    # TODO: Iterate through documents, chunk each one, and add metadata\n",
        "    #\n",
        "    # For each document:\n",
        "    for doc in documents[:1]:\n",
        "        #   1. Get text and source from document dict\n",
        "        text = doc[\"text\"]\n",
        "        source = doc[\"source\"]\n",
        "        #   2. Call chunk_document() to get chunks\n",
        "        chunks = chunk_document(text, chunk_size, chunk_overlap)\n",
        "        #   3. For each chunk, create a dict with chunk_id, text, and source\n",
        "        for chunk in chunks:\n",
        "            tdict = {}\n",
        "            tdict[\"chunk_id\"] = chunk_id\n",
        "            chunk_id += 1\n",
        "            tdict[\"text\"] = chunk\n",
        "            tdict[\"source\"] = source\n",
        "\n",
        "            #   4. Append to all_chunks and increment chunk_id\n",
        "            all_chunks.append(tdict)\n",
        "\n",
        "    return all_chunks\n",
        "\n",
        "\n",
        "# Create chunks from all documents\n",
        "CHUNK_SIZE = 1000\n",
        "CHUNK_OVERLAP = 200\n",
        "\n",
        "chunks = chunk_all_documents(documents, CHUNK_SIZE, CHUNK_OVERLAP)\n",
        "\n",
        "print(f\"\\nCreated {len(chunks)} chunks from {len(documents)} documents\")\n",
        "print(f\"Average chunks per document: {len(chunks) / len(documents):.2f}\")\n",
        "\n",
        "# Show sample chunk\n",
        "sid = np.random.randint(0, len(chunks))\n",
        "if chunks:\n",
        "    print(f\"\\nSample chunk:\")\n",
        "    print(f\"  ID: {chunks[sid]['chunk_id']}\")\n",
        "    print(f\"  Source: {chunks[sid]['source']}\")\n",
        "    print(f\"  Text (first 200 chars): {chunks[sid]['text'][:200]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6D6kNKbW_Mnl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test: 2500 char text with chunk_size=1000, overlap=200\n",
            "Expected chunks: ~4\n",
            "Your chunks: 3\n",
            "✅ Chunking test passed!\n"
          ]
        }
      ],
      "source": [
        "# Test your chunking implementation\n",
        "test_text = \"A\" * 2500  # 2500 characters\n",
        "test_chunks = chunk_document(test_text, chunk_size=1000, chunk_overlap=200)\n",
        "\n",
        "print(f\"Test: 2500 char text with chunk_size=1000, overlap=200\")\n",
        "print(f\"Expected chunks: ~4\")\n",
        "print(f\"Your chunks: {len(test_chunks)}\")\n",
        "\n",
        "if len(test_chunks) >= 3 and len(test_chunks) <= 5:\n",
        "    print(\"✅ Chunking test passed!\")\n",
        "else:\n",
        "    print(\"❌ Check your chunking implementation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "llntJu71_Mnl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Created 3 chunks from 500 documents\n",
            "Average chunks per document: 0.01\n",
            "\n",
            "Sample chunk:\n",
            "  ID: 0\n",
            "  Source: huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/create_endpoint.mdx\n",
            "  Text (first 200 chars):  Create an Endpoint\n",
            "\n",
            "After your first login, you will be directed to the [Endpoint creation page](https://ui.endpoints.huggingface.co/new). As an example, this guide will go through the steps to deplo...\n"
          ]
        }
      ],
      "source": [
        "# Create chunks from all documents\n",
        "CHUNK_SIZE = 1000\n",
        "CHUNK_OVERLAP = 200\n",
        "\n",
        "chunks = chunk_all_documents(documents, CHUNK_SIZE, CHUNK_OVERLAP)\n",
        "\n",
        "print(f\"\\nCreated {len(chunks)} chunks from {len(documents)} documents\")\n",
        "print(f\"Average chunks per document: {len(chunks) / len(documents):.2f}\")\n",
        "\n",
        "# Show sample chunk\n",
        "if chunks:\n",
        "    print(f\"\\nSample chunk:\")\n",
        "    print(f\"  ID: {chunks[0]['chunk_id']}\")\n",
        "    print(f\"  Source: {chunks[0]['source']}\")\n",
        "    print(f\"  Text (first 200 chars): {chunks[0]['text'][:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_Tn_ML1_Mnl"
      },
      "source": [
        "## 4. Embeddings\n",
        "\n",
        "Generate vector embeddings for each chunk using BGE-small-en-v1.5.\n",
        "\n",
        "### Your Task\n",
        "\n",
        "Implement the `generate_embeddings` function that:\n",
        "\n",
        "1. Processes texts in batches for memory efficiency\n",
        "2. Uses the SentenceTransformer model to generate embeddings\n",
        "3. Returns embeddings as a list of lists (for Milvus compatibility)\n",
        "\n",
        "### Hints\n",
        "\n",
        "- Use `model.encode()` with `normalize_embeddings=True` for cosine similarity\n",
        "- Process in batches to avoid memory issues\n",
        "- Convert numpy arrays to lists using `.tolist()`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Lqh6daOW_Mnl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded embedding model: BAAI/bge-small-en-v1.5\n",
            "Embedding dimension: 384 , test_embedding.shape=(2, 384)\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load the embedding model\n",
        "EMBEDDING_MODEL = (\n",
        "    \"BAAI/bge-small-en-v1.5\"  # Use any model of your choice from Sentence Transformers\n",
        ")\n",
        "embedding_model = SentenceTransformer(EMBEDDING_MODEL)\n",
        "\n",
        "print(f\"Loaded embedding model: {EMBEDDING_MODEL}\")\n",
        "\n",
        "# Test embedding\n",
        "test_embedding = embedding_model.encode(\n",
        "    [\"This is a test\", \"This is another test\"], normalize_embeddings=True\n",
        ")\n",
        "EMBEDDING_DIM = len(test_embedding[0])\n",
        "print(f\"Embedding dimension: {EMBEDDING_DIM} , {test_embedding.shape=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "3\n",
            "6\n",
            "9\n",
            "12\n",
            "15\n",
            "18\n"
          ]
        }
      ],
      "source": [
        "for i in range(0, 20, 3):\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "jLJoa2-r_Mnm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated 3 embeddings\n",
            "Embedding dimension: 384\n",
            "✅ Embedding generation test passed!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# TODO: IMPLEMENT EMBEDDING GENERATION (15 points)\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "def generate_embeddings(\n",
        "    texts: List[str], model: SentenceTransformer, batch_size: int = 32\n",
        ") -> List[List[float]]:\n",
        "    \"\"\"\n",
        "    Generate embeddings for a list of texts.\n",
        "\n",
        "    Args:\n",
        "        texts: List of text strings to embed\n",
        "        model: SentenceTransformer model\n",
        "        batch_size: Number of texts to process at once\n",
        "\n",
        "    Returns:\n",
        "        List of embedding vectors (as lists of floats)\n",
        "    \"\"\"\n",
        "    all_embeddings = []\n",
        "    # Step 1: Loop through texts in batches of size batch_size\n",
        "\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        btext = texts[i : i + batch_size]\n",
        "        bembs = model.encode(btext, normalize_embeddings=True)\n",
        "        all_embeddings.extend(bembs.tolist())\n",
        "\n",
        "    # Step 2: For each batch, call model.encode() with:\n",
        "    #   - The batch of texts\n",
        "    #   - normalize_embeddings=True (important for cosine similarity)\n",
        "    #   - show_progress_bar=False (we use tqdm at the outer level)\n",
        "\n",
        "    return all_embeddings\n",
        "\n",
        "    # Step 3: Convert to list format and extend all_embeddings\n",
        "\n",
        "\n",
        "# Test your embedding generation\n",
        "test_texts = [\"Hello world\", \"This is a test\", \"RAG is cool\"]\n",
        "test_embeddings = generate_embeddings(test_texts, embedding_model)\n",
        "\n",
        "print(f\"Generated {len(test_embeddings)} embeddings\")\n",
        "print(f\"Embedding dimension: {len(test_embeddings[0]) if test_embeddings else 0}\")\n",
        "\n",
        "if len(test_embeddings) == 3 and len(test_embeddings[0]) == 384:\n",
        "    print(\"✅ Embedding generation test passed!\")\n",
        "else:\n",
        "    print(\"❌ Check your embedding implementation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "cd58JLfI_Mnm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated 3 embeddings\n",
            "Embedding dimension: 384\n",
            "✅ Embedding generation test passed!\n"
          ]
        }
      ],
      "source": [
        "# Test your embedding generation\n",
        "test_texts = [\"Hello world\", \"This is a test\", \"RAG is cool\"]\n",
        "test_embeddings = generate_embeddings(test_texts, embedding_model)\n",
        "\n",
        "print(f\"Generated {len(test_embeddings)} embeddings\")\n",
        "print(f\"Embedding dimension: {len(test_embeddings[0]) if test_embeddings else 0}\")\n",
        "\n",
        "if len(test_embeddings) == 3 and len(test_embeddings[0]) == 384:\n",
        "    print(\"✅ Embedding generation test passed!\")\n",
        "else:\n",
        "    print(\"❌ Check your embedding implementation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "OreJHVF2_Mnm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generated 3 embeddings\n",
            "Embedding dimension: 384\n",
            "Sample embedding (first 10 values): [-0.07532960921525955, -0.027508024126291275, -0.03995616361498833, -0.04049210995435715, 0.03333999961614609, 0.042965102940797806, -0.04333630949258804, -0.044938262552022934, -0.05554322153329849, 0.026720287278294563]\n"
          ]
        }
      ],
      "source": [
        "# Generate embeddings for all chunks\n",
        "chunk_texts = [chunk[\"text\"] for chunk in chunks]\n",
        "embeddings = generate_embeddings(chunk_texts, embedding_model)\n",
        "\n",
        "print(f\"\\nGenerated {len(embeddings)} embeddings\")\n",
        "if embeddings:\n",
        "    print(f\"Embedding dimension: {len(embeddings[0])}\")\n",
        "    print(f\"Sample embedding (first 10 values): {embeddings[0][:10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qpQmJzk_Mnm"
      },
      "source": [
        "## 5. Vector Store (Milvus)\n",
        "\n",
        "Store embeddings in Milvus for efficient similarity search.\n",
        "\n",
        "### Your Task\n",
        "\n",
        "1. Implement `setup_milvus_collection` to create a new collection\n",
        "2. Implement `insert_data_to_milvus` to insert chunks and embeddings\n",
        "\n",
        "### Hints\n",
        "\n",
        "- Use `client.has_collection()` to check if collection exists\n",
        "- Use `client.drop_collection()` to remove existing collection\n",
        "- Use `client.create_collection()` with dimension and metric_type parameters\n",
        "- Use `client.insert()` to add data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwIcljiX_Mnm"
      },
      "outputs": [],
      "source": [
        "from pymilvus import MilvusClient\n",
        "\n",
        "# Initialize Milvus client (uses Milvus Lite - stores data locally)\n",
        "MILVUS_DB_PATH = \"./hf_docs_milvus.db\"\n",
        "milvus_client = MilvusClient(uri=MILVUS_DB_PATH)\n",
        "\n",
        "COLLECTION_NAME = \"hf_documentation\"\n",
        "\n",
        "print(f\"Milvus client initialized with database: {MILVUS_DB_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMAHuGsp_Mnm"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# TODO: IMPLEMENT MILVUS COLLECTION SETUP (10 points)\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "def setup_milvus_collection(\n",
        "    client: MilvusClient, collection_name: str, embedding_dim: int\n",
        "):\n",
        "    \"\"\"\n",
        "    Create a Milvus collection for storing document embeddings.\n",
        "\n",
        "    Args:\n",
        "        client: MilvusClient instance\n",
        "        collection_name: Name of the collection to create\n",
        "        embedding_dim: Dimension of the embedding vectors\n",
        "    \"\"\"\n",
        "    # TODO: Create a Milvus collection\n",
        "    #\n",
        "    # Step 1: Check if collection already exists using client.has_collection()\n",
        "    # Step 2: If exists, drop it using client.drop_collection()\n",
        "    # Step 3: Create new collection using client.create_collection() with:\n",
        "    #   - collection_name: the name parameter\n",
        "    #   - dimension: embedding_dim parameter\n",
        "    #   - metric_type: \"IP\" (Inner Product for cosine similarity)\n",
        "    #   - consistency_level: \"Strong\"\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    pass\n",
        "\n",
        "    print(f\"Created collection: {collection_name} with dimension {embedding_dim}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtpqSzLd_Mnm"
      },
      "outputs": [],
      "source": [
        "# Setup the collection\n",
        "setup_milvus_collection(milvus_client, COLLECTION_NAME, EMBEDDING_DIM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-a17gp9U_Mnm"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# TODO: IMPLEMENT DATA INSERTION (10 points)\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "def insert_data_to_milvus(\n",
        "    client: MilvusClient,\n",
        "    collection_name: str,\n",
        "    chunks: List[Dict],\n",
        "    embeddings: List[List[float]],\n",
        "    batch_size: int = 100,\n",
        "):\n",
        "    \"\"\"\n",
        "    Insert document chunks and embeddings into Milvus.\n",
        "\n",
        "    Args:\n",
        "        client: MilvusClient instance\n",
        "        collection_name: Name of the collection\n",
        "        chunks: List of chunk dictionaries with text and metadata\n",
        "        embeddings: List of embedding vectors\n",
        "        batch_size: Number of records to insert at once\n",
        "\n",
        "    Returns:\n",
        "        Total number of inserted records\n",
        "    \"\"\"\n",
        "    total_inserted = 0\n",
        "\n",
        "    # TODO: Insert data into Milvus\n",
        "    #\n",
        "    # Step 1: Prepare data as a list of dictionaries, where each dict has:\n",
        "    #   - \"id\": chunk[\"chunk_id\"]\n",
        "    #   - \"vector\": the corresponding embedding\n",
        "    #   - \"text\": chunk[\"text\"]\n",
        "    #   - \"source\": chunk[\"source\"]\n",
        "\n",
        "    # Step 2: Insert in batches using client.insert()\n",
        "    # Hint: Loop through data in batches and call:\n",
        "    #   result = client.insert(collection_name=collection_name, data=batch)\n",
        "    #   total_inserted += result[\"insert_count\"]\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    pass\n",
        "\n",
        "    return total_inserted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTzQZmUl_Mnm"
      },
      "outputs": [],
      "source": [
        "# Insert data into Milvus\n",
        "inserted_count = insert_data_to_milvus(\n",
        "    milvus_client, COLLECTION_NAME, chunks, embeddings\n",
        ")\n",
        "\n",
        "print(f\"\\nInserted {inserted_count} records into Milvus\")\n",
        "\n",
        "if inserted_count == len(chunks):\n",
        "    print(\"✅ All chunks inserted successfully!\")\n",
        "else:\n",
        "    print(\"❌ Not all chunks were inserted. Check your implementation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1isFIiD_Mnm"
      },
      "source": [
        "## 6. Retrieval\n",
        "\n",
        "Implement semantic search to retrieve relevant documents for a query.\n",
        "\n",
        "### Your Task\n",
        "\n",
        "Implement the `retrieve_documents` function that:\n",
        "\n",
        "1. Generates an embedding for the query\n",
        "2. Searches Milvus for similar vectors\n",
        "3. Returns the top-K most relevant documents\n",
        "\n",
        "### Hints\n",
        "\n",
        "- Use `embedding_model.encode()` to embed the query\n",
        "- Use `client.search()` to find similar vectors\n",
        "- Extract text and source from the search results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfwXdwGO_Mnm"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# TODO: IMPLEMENT RETRIEVAL (25 points)\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "def retrieve_documents(\n",
        "    query: str,\n",
        "    client: MilvusClient,\n",
        "    collection_name: str,\n",
        "    embedding_model: SentenceTransformer,\n",
        "    top_k: int = 5,\n",
        ") -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Retrieve the most relevant documents for a query.\n",
        "\n",
        "    Args:\n",
        "        query: The search query\n",
        "        client: MilvusClient instance\n",
        "        collection_name: Name of the collection to search\n",
        "        embedding_model: Model to generate query embedding\n",
        "        top_k: Number of results to return\n",
        "\n",
        "    Returns:\n",
        "        List of dictionaries with 'text', 'source', and 'score' keys\n",
        "    \"\"\"\n",
        "    # TODO: Implement semantic search\n",
        "    #\n",
        "    # Step 1: Generate embedding for the query\n",
        "    # Hint: Use embedding_model.encode([query], normalize_embeddings=True)\n",
        "    #       Then convert to list: .tolist()[0]\n",
        "\n",
        "    # Step 2: Search in Milvus using client.search()\n",
        "    # Required parameters:\n",
        "    #   - collection_name: collection_name\n",
        "    #   - data: [query_embedding] (list containing the embedding)\n",
        "    #   - limit: top_k\n",
        "    #   - search_params: {\"metric_type\": \"IP\", \"params\": {}}\n",
        "    #   - output_fields: [\"text\", \"source\"]\n",
        "\n",
        "    # Step 3: Format results as list of dicts\n",
        "    # Each dict should have:\n",
        "    #   - \"text\": result[\"entity\"][\"text\"]\n",
        "    #   - \"source\": result[\"entity\"][\"source\"]\n",
        "    #   - \"score\": result[\"distance\"]\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    retrieved_docs = []\n",
        "\n",
        "    return retrieved_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Es39BSL_Mnn"
      },
      "outputs": [],
      "source": [
        "# Test retrieval\n",
        "test_query = \"How do I fine-tune a transformer model?\"\n",
        "\n",
        "retrieved = retrieve_documents(\n",
        "    query=test_query,\n",
        "    client=milvus_client,\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    embedding_model=embedding_model,\n",
        "    top_k=3,\n",
        ")\n",
        "\n",
        "print(f\"Query: {test_query}\")\n",
        "print(f\"\\nRetrieved {len(retrieved)} documents:\")\n",
        "for i, doc in enumerate(retrieved):\n",
        "    print(f\"\\n--- Document {i+1} (Score: {doc.get('score', 'N/A')}) ---\")\n",
        "    print(f\"Source: {doc.get('source', 'N/A')}\")\n",
        "    print(f\"Text: {doc.get('text', 'N/A')[:300]}...\")\n",
        "\n",
        "if len(retrieved) == 3 and all(\"text\" in d for d in retrieved):\n",
        "    print(\"\\n✅ Retrieval test passed!\")\n",
        "else:\n",
        "    print(\"\\n❌ Check your retrieval implementation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUm-anJf_Mnn"
      },
      "source": [
        "## 7. Generation\n",
        "\n",
        "Generate answers using Microsoft Phi-3-mini-4k-instruct/Qwen.\n",
        "\n",
        "### Your Task\n",
        "\n",
        "Implement the `generate_answer` function that:\n",
        "\n",
        "1. Combines retrieved documents into a context string\n",
        "2. Formats the prompt using the provided template\n",
        "3. Generates an answer using the language model\n",
        "4. Returns a structured result dictionary\n",
        "\n",
        "### Hints\n",
        "\n",
        "- Join document texts with newlines to create context\n",
        "- Use the PROMPT_TEMPLATE.format() to fill in context and question\n",
        "- Call the generator pipeline with appropriate parameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlDeWGZ9JDfM"
      },
      "source": [
        "### https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8w3YXOaJJ4N"
      },
      "source": [
        "### https://huggingface.co/microsoft/Phi-3.5-mini-instruct\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-Z9rpiGJPYW"
      },
      "source": [
        "### https://huggingface.co/Qwen/Qwen2-1.5B-Instruct\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0ieMxESJRZh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBamzDbZKOpK"
      },
      "source": [
        "### FEEL FREE TO USE A PROPRIETARY MODEL LIKE OPENAI, CLAUDE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "0o3E_QXR_Mnn",
        "outputId": "df58ac56-efc6-4c8f-c1e4-6e6f7d61696c"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-373466297.py, line 5)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-373466297.py\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    LLM_MODEL = #SPECIFY THE MODEL FROM HUGGINGFACE\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "\n",
        "# Load the language model\n",
        "LLM_MODEL = #SPECIFY THE MODEL FROM HUGGINGFACE\n",
        "\n",
        "print(f\"Loading model: {LLM_MODEL}\")\n",
        "print(\"This may take a few minutes...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    LLM_MODEL,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Create text generation pipeline\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "print(f\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq4ek9pfAS2Q"
      },
      "source": [
        "### MODIFY THIS TO SUIT YOUR MODEL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OCUmNvk_Mnn"
      },
      "outputs": [],
      "source": [
        "# Prompt template for RAG (YOU ARE FREE TO MODIFY)\n",
        "PROMPT_TEMPLATE = \"\"\"Use the following pieces of information enclosed in <context> tags to provide an answer to the question enclosed in <question> tags.\n",
        "If the context doesn't contain enough information to answer the question, say \"I don't have enough information to answer this question.\"\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "<question>\n",
        "{question}\n",
        "</question>\n",
        "\n",
        "Answer:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ct-0KSE_Mnn"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# TODO: IMPLEMENT GENERATION (25 points)\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "def generate_answer(\n",
        "    query: str,\n",
        "    retrieved_docs: List[Dict],\n",
        "    generator: pipeline,\n",
        "    max_new_tokens: int = 256,\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Generate an answer using retrieved documents as context.\n",
        "\n",
        "    Args:\n",
        "        query: The user's question\n",
        "        retrieved_docs: List of retrieved document dictionaries\n",
        "        generator: HuggingFace text generation pipeline\n",
        "        max_new_tokens: Maximum tokens to generate\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with 'answer', 'context', 'query', and 'retrieved_docs'\n",
        "    \"\"\"\n",
        "    # TODO: Generate an answer using the RAG pattern\n",
        "    #\n",
        "    # Step 1: Combine retrieved documents into context\n",
        "    # Hint: Join doc[\"text\"] for each doc with \"\\n\\n\" separator\n",
        "\n",
        "    # Step 2: Format the prompt using PROMPT_TEMPLATE\n",
        "    # Hint: prompt = PROMPT_TEMPLATE.format(context=context, question=query)\n",
        "\n",
        "    # Step 3: Generate response using the generator pipeline\n",
        "    # Call generator() with:\n",
        "    #   - prompt (first argument)\n",
        "    #   - max_new_tokens=max_new_tokens\n",
        "    #   - do_sample=True\n",
        "    #   - temperature=0.7\n",
        "    #   - top_p=0.9\n",
        "    #   - return_full_text=False\n",
        "\n",
        "    # Step 4: Extract the generated text\n",
        "    # Hint: outputs[0][\"generated_text\"].strip()\n",
        "\n",
        "    # Step 5: Return result dictionary\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    context = \"\"\n",
        "    answer = \"\"\n",
        "\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"answer\": answer,\n",
        "        \"context\": context,\n",
        "        \"retrieved_docs\": retrieved_docs,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9pr48cM_Mnn"
      },
      "outputs": [],
      "source": [
        "# Test generation\n",
        "test_query = \"How do I fine-tune a transformer model?\"\n",
        "\n",
        "# Retrieve relevant documents\n",
        "retrieved = retrieve_documents(\n",
        "    query=test_query,\n",
        "    client=milvus_client,\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    embedding_model=embedding_model,\n",
        "    top_k=3,\n",
        ")\n",
        "\n",
        "# Generate answer\n",
        "result = generate_answer(\n",
        "    query=test_query, retrieved_docs=retrieved, generator=generator\n",
        ")\n",
        "\n",
        "print(f\"Question: {result['query']}\")\n",
        "print(f\"\\nAnswer: {result['answer']}\")\n",
        "\n",
        "if result[\"answer\"] and len(result[\"answer\"]) > 10:\n",
        "    print(\"\\n✅ Generation test passed!\")\n",
        "else:\n",
        "    print(\"\\n❌ Check your generation implementation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KveAEzm_Mnn"
      },
      "outputs": [],
      "source": [
        "# Complete RAG pipeline function (DO NOT MODIFY)\n",
        "\n",
        "\n",
        "def rag_query(\n",
        "    query: str,\n",
        "    client: MilvusClient,\n",
        "    collection_name: str,\n",
        "    embedding_model: SentenceTransformer,\n",
        "    generator: pipeline,\n",
        "    top_k: int = 5,\n",
        "    max_new_tokens: int = 256,\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Complete RAG pipeline: retrieve then generate.\n",
        "    \"\"\"\n",
        "    # Retrieve\n",
        "    retrieved_docs = retrieve_documents(\n",
        "        query=query,\n",
        "        client=client,\n",
        "        collection_name=collection_name,\n",
        "        embedding_model=embedding_model,\n",
        "        top_k=top_k,\n",
        "    )\n",
        "\n",
        "    # Generate\n",
        "    result = generate_answer(\n",
        "        query=query,\n",
        "        retrieved_docs=retrieved_docs,\n",
        "        generator=generator,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "    )\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDRlGGxQ_Mnn"
      },
      "outputs": [],
      "source": [
        "# Test complete pipeline with multiple queries\n",
        "test_queries = [\n",
        "    \"What is the Trainer class in transformers?\",\n",
        "    \"How do I load a dataset from HuggingFace?\",\n",
        "    \"What is Gradio used for?\",\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    result = rag_query(\n",
        "        query=query,\n",
        "        client=milvus_client,\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        embedding_model=embedding_model,\n",
        "        generator=generator,\n",
        "        top_k=3,\n",
        "    )\n",
        "    print(f\"Q: {result['query']}\")\n",
        "    print(f\"A: {result['answer']}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
