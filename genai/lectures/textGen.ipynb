{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1c7221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color = sns.color_palette('dark'))\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = (10 , 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61c180e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client connected successfully!\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# os.environ['OPENAI_API_KEY'] = 'none'\n",
    "\n",
    "client = OpenAI()\n",
    "print('OpenAI client connected successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1152a47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "GENERATION MODEL (GPT-4o-mini)\n",
      "======================================================================\n",
      "Prompt strategy: Explicit step-by-step scaffolding\n",
      "Engineer's role: Design the reasoning chain\n",
      "Token cost: Lower per call\n",
      "\n",
      "======================================================================\n",
      "REASONING MODEL (o1-mini)\n",
      "======================================================================\n",
      "Prompt strategy: State goal + constraints\n",
      "Engineer's role: Define success criteria\n",
      "Token cost: Higher per call, but more reliable logic\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def loan_decision_with_generation_model(applicant_data):\n",
    "    \"\"\"\n",
    "    Using GPT-4o-mini (generation architecture)\n",
    "    Requires explicit decomposition of the decision process\n",
    "    \"\"\"\n",
    "\n",
    "    structured_prompt = f\"\"\"\n",
    "You are evaluating a loan application. Follow these exact steps:\n",
    "\n",
    "STEP 1 - Compute debt-to-income ratio:\n",
    "Monthly debt: ${applicant_data['debt']}\n",
    "Monthly income: ${applicant_data['income']}\n",
    "Formula: debt ÷ income\n",
    "Your calculation: _____\n",
    "\n",
    "STEP 2 - Classify credit tier:\n",
    "Score: {applicant_data['credit_score']}\n",
    "Tiers:\n",
    "  740+: Premium\n",
    "  670-739: Standard\n",
    "  580-669: Subprime\n",
    "  <580: High-risk\n",
    "Classification: _____\n",
    "\n",
    "STEP 3 - Verify employment:\n",
    "Months employed: {applicant_data['employment_months']}\n",
    "Minimum required: 24 months\n",
    "Income stability (variance): {applicant_data['income_variance']}%\n",
    "Maximum allowed: 15%\n",
    "Status: _____\n",
    "\n",
    "STEP 4 - Assess collateral:\n",
    "Property value: ${applicant_data['collateral']}\n",
    "Loan amount: ${applicant_data['loan_amount']}\n",
    "LTV ratio: _____\n",
    "\n",
    "[... would continue for all decision criteria ...]\n",
    "\n",
    "FINAL OUTPUT: Approved/Denied with justification\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": structured_prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def loan_decision_with_reasoning_model(applicant_data):\n",
    "    \"\"\"\n",
    "    Using o1-mini (reasoning architecture)\n",
    "    Model performs internal decomposition automatically\n",
    "    \"\"\"\n",
    "\n",
    "    minimal_prompt = f\"\"\"\n",
    "Evaluate this loan application against our criteria:\n",
    "\n",
    "Applicant Data:\n",
    "- Monthly debt: ${applicant_data['debt']}\n",
    "- Monthly income: ${applicant_data['income']}\n",
    "- Credit score: {applicant_data['credit_score']}\n",
    "- Employment: {applicant_data['employment_months']} months\n",
    "- Income variance: {applicant_data['income_variance']}%\n",
    "- Collateral value: ${applicant_data['collateral']}\n",
    "- Requested amount: ${applicant_data['loan_amount']}\n",
    "\n",
    "Approval Criteria:\n",
    "- Debt-to-income < 43%\n",
    "- Credit score ≥ 580\n",
    "- Employment ≥ 24 months with income variance < 15%\n",
    "- Adequate collateral coverage\n",
    "\n",
    "Decision: Approve or deny? Explain your reasoning.\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"o1\",\n",
    "        messages=[{\"role\": \"user\", \"content\": minimal_prompt}]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# Example applicant\n",
    "test_application = {\n",
    "    'debt': 2800,\n",
    "    'income': 7500,\n",
    "    'credit_score': 695,\n",
    "    'employment_months': 30,\n",
    "    'income_variance': 12.5,\n",
    "    'collateral': 180000,\n",
    "    'loan_amount': 140000\n",
    "}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GENERATION MODEL (GPT-4o-mini)\")\n",
    "print(\"=\"*70)\n",
    "print(\"Prompt strategy: Explicit step-by-step scaffolding\")\n",
    "print(\"Engineer's role: Design the reasoning chain\")\n",
    "print(\"Token cost: Lower per call\")\n",
    "print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"REASONING MODEL (o1-mini)\")\n",
    "print(\"=\"*70)\n",
    "print(\"Prompt strategy: State goal + constraints\")\n",
    "print(\"Engineer's role: Define success criteria\")\n",
    "print(\"Token cost: Higher per call, but more reliable logic\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac9a4e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/torch/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a54f601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2061,  318,  262, 3139,  286, 4881, 5633]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "prompt = 'What is the capital of France ?'\n",
    "tokens = tokenizer.encode(prompt , return_tensors = 'pt')\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ddab952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec30057b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2 output: What is the capital of France ? It is the capital of France . . . We do not say that the capital of France is French.\"\n",
      "\n",
      "\"The capital of France is French. It is the capital of France . . . We do not say that the capital of France is\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(\n",
    "    tokens , max_new_tokens = 50,\n",
    "    do_sample = True,\n",
    "    temperature = 0.7,\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    ")\n",
    "print(f\"{model_name} output: {tokenizer.decode(output[0] , skip_special_tokens= True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6da7eb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Translate 'Hello' to French:\n",
      "Output: Translate 'Hello' to French:\n",
      "\n",
      "The German translation is:\n",
      "\n",
      "Hello!\n",
      "\n",
      "This is a special version of the German version.\n",
      "\n",
      "The English translation is:\n",
      "\n",
      "--------------------------------------------------\n",
      "Prompt: Write a poem about the ocean:\n",
      "Output: Write a poem about the ocean: \"I was living through a wave of a global tsunami. I was watching a lot of things happening in the ocean. I heard the waves and thought\n",
      "\n",
      "--------------------------------------------------\n",
      "Prompt: Summarize the following: The quick brown fox jumps over the lazy dog.\n",
      "Output: Summarize the following: The quick brown fox jumps over the lazy dog.\n",
      "\n",
      "The quick brown fox jumps over the lazy dog. The quick fox jumps over the lazy dog. The quick fox jumps over the lazy dog.\n",
      "\n",
      "--------------------------------------------------\n",
      "Prompt: Answer the following question: What is 2+2?\n",
      "Output: Answer the following question: What is 2+2?\n",
      "\n",
      "Answer: The 2+2 factor is the number of points to give to an attacker. The attacker only takes one point out of the 2\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Translate 'Hello' to French:\",\n",
    "    \"Write a poem about the ocean:\",\n",
    "    \"Summarize the following: The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Answer the following question: What is 2+2?\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=30,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Output: {tokenizer.decode(output[0], skip_special_tokens=True)}\\n\")\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7c50c1",
   "metadata": {},
   "source": [
    "## Instruction Model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91d1b548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Load an instruct model\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Same prompt as before - using instruction format\n",
    "prompt = \"What is the capital of France?\"\n",
    "formatted_prompt = f\"<|user|>\\n{prompt}</s>\\n<|assistant|>\\n\"\n",
    "\n",
    "output = generator(\n",
    "    formatted_prompt,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0cffb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of France?\n",
      "Response: Yes, the capital of France is Paris. However, the official name of the city as well as the seat of the country's government is the City of Paris. The city of Paris is divided into five arrondissements (districts\n"
     ]
    }
   ],
   "source": [
    "print(f\"{prompt}\")\n",
    "print(f\"Response: {output[0]['generated_text'].split('<|assistant|>')[-1].strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c2bdc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Austria is Vienna.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of Austria?\"},\n",
    "]\n",
    "output = generator(messages , max_new_tokens = 50)\n",
    "print(f\"{output[0]['generated_text'][-1]['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5815aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is 5 + 3?\n",
      "Assistant: Yes, I can help you with that.\n",
      "\n",
      "5 + 3 = 8\n",
      "\n",
      "The expression \"5 + 3\" represents addition of 5 and 3, which equals 8.\n",
      "\n",
      "User: Now multiply that result by 2.\n",
      "Assistant: Yes, I can help you with that.\n",
      "\n",
      "8 * 2 = 16\n",
      "\n",
      "The expression \"8 * 2\" represents multiplication of 8 and 2, which equals 16.\n",
      "\n",
      "User: What if I divide that by 4?\n",
      "Assistant: Sure, I can help you with that.\n",
      "\n",
      "16 / 4 = 4\n",
      "\n",
      "The expression \"16 / 4\" represents division of 16 by 4, which equals 4.\n"
     ]
    }
   ],
   "source": [
    "# Multi-turn conversation demonstration\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful math tutor.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is 5 + 3?\"},\n",
    "]\n",
    "\n",
    "# First turn\n",
    "output = generator(messages, max_new_tokens=50)\n",
    "assistant_response = output[0]['generated_text'][-1]['content']\n",
    "print(f\"User: What is 5 + 3?\")\n",
    "print(f\"Assistant: {assistant_response}\\n\")\n",
    "\n",
    "# Add assistant response to history\n",
    "messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "\n",
    "# Second turn - referring to previous context\n",
    "messages.append({\"role\": \"user\", \"content\": \"Now multiply that result by 2.\"})\n",
    "output = generator(messages, max_new_tokens=50)\n",
    "assistant_response = output[0]['generated_text'][-1]['content']\n",
    "print(f\"User: Now multiply that result by 2.\")\n",
    "print(f\"Assistant: {assistant_response}\\n\")\n",
    "\n",
    "# Third turn\n",
    "messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "messages.append({\"role\": \"user\", \"content\": \"What if I divide that by 4?\"})\n",
    "output = generator(messages, max_new_tokens=50)\n",
    "assistant_response = output[0]['generated_text'][-1]['content']\n",
    "print(f\"User: What if I divide that by 4?\")\n",
    "print(f\"Assistant: {assistant_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc6e72a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
