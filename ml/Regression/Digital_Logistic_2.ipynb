{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Content\n",
        "\n",
        "\n",
        "- Hyperparameter Tuning\n",
        "\n",
        "- LogOdds and Logit\n",
        "  - [Odds of tails](https://www.scaler.com/hire/test/problem/23537/)\n",
        "  - [Odds of Drinking](https://www.scaler.com/hire/test/problem/20190/)\n",
        "  - [Red or Black Jacket](https://www.scaler.com/hire/test/problem/23572/)\n",
        "  - [Implement the model](https://www.scaler.com/hire/test/problem/23535/) (hold)\n",
        "  - [Sigmoid Function and logistic model](https://www.scaler.com/hire/test/problem/20188/) (hold)\n",
        "\n",
        "- Impact of outliers\n",
        "  - [Impact of Outliers on Logistic Regression](https://www.scaler.com/hire/test/problem/29675/)\n",
        "\n",
        "- Multiclass classification\n",
        "  - [One-vs-Rest](https://www.scaler.com/hire/test/problem/24776/) (hold)\n",
        "  - [Multiclass classification II](https://www.scaler.com/hire/test/problem/24777/)\n",
        "  - [Fruits/Vegetables](https://www.scaler.com/hire/test/problem/20238/)\n",
        "  - [One Vs Rest MCQ](https://www.scaler.com/hire/test/problem/29671/)\n",
        "  - [One vs Rest](https://www.scaler.com/hire/test/problem/24778/)\n",
        "\n",
        "- Extra (HW):\n",
        "  -[Logistic regression assumptions](https://www.scaler.com/hire/test/problem/16045/)\n"
      ],
      "metadata": {
        "id": "HDyWZxUqglU6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how we can perform hyperparameter tuning on our logistic regression model\n"
      ],
      "metadata": {
        "id": "SnlIucWFUMSG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='red'>Instructor Note</font>\n",
        "\n",
        "If possible, show the documentation page of sklearn's logistic regression, and explain few parameters before going into hyperparameter tuning"
      ],
      "metadata": {
        "id": "lo6_gT65-h0w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Hyperparameter tuning**"
      ],
      "metadata": {
        "id": "QagbDTn3N0LG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will tune the regularization rate of our model.\n",
        "\n",
        "You can refer to the documentation for the various list of parameters in logistic regression.\n",
        "\n",
        "Link: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n"
      ],
      "metadata": {
        "id": "Zc8N5L_YNNPH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Hence let's start doing hyper parameter tuning on parameter $C = \\frac{1}{\\lambda}$  to increase the performance of the model"
      ],
      "metadata": {
        "id": "yuxZwtpZNz5Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVBtRLrTbMA2",
        "outputId": "74ae7189-7ebe-4d8b-ba91-c9e0b1cf3f12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'StandardScaler' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-801da0a7cf56>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mval_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mla\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5000.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# range of values of Lambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mscaled_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'StandardScaler' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "train_scores = []\n",
        "val_scores = []\n",
        "scaler = StandardScaler()\n",
        "for la in np.arange(0.01, 5000.0, 100): # range of values of Lambda\n",
        "  scaled_lr = make_pipeline(scaler, LogisticRegression(C=1/la))\n",
        "  scaled_lr.fit(X_train, y_train)\n",
        "  train_score = accuracy(y_train, scaled_lr.predict(X_train))\n",
        "  val_score = accuracy(y_val, scaled_lr.predict(X_val))\n",
        "  train_scores.append(train_score)\n",
        "  val_scores.append(val_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-4WX0BUcezZ"
      },
      "source": [
        "Now, let's plot the graph and pick the Regularization Parameter $λ$ which gives the best validation score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_u1uS8_AcrMp"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(list(np.arange(0.01, 5000.0, 100)), train_scores, label=\"train\")\n",
        "plt.plot(list(np.arange(0.01, 5000.0, 100)), val_scores, label=\"val\")\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "plt.xlabel(\"Regularization Parameter(λ)\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjgrgrSuKSAq"
      },
      "source": [
        "\n",
        "- We see how Validation increases to a peak and then decreases\n",
        "\n",
        "- Notice as Regularization is increasing, the Accuracy decreasing since model is moving towards Underfit"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take lambda value as 1000 for this data and check the\n",
        "results"
      ],
      "metadata": {
        "id": "iiChnCjdV22_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(C=1/1000)\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "qGY7gojXPCoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy(y_train, model.predict(X_train))"
      ],
      "metadata": {
        "id": "JCkfn5ycPUBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy(y_val, model.predict(X_val))"
      ],
      "metadata": {
        "id": "_txvnm0xPUBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe an increase of 0.01, or 1%, in both training and validation data\n",
        "\n",
        "Let's check our model for test data too"
      ],
      "metadata": {
        "id": "AKs4aurQPnO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy(y_test, model.predict(X_test))"
      ],
      "metadata": {
        "id": "4j-b26k9PWGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quiz** (Check your understanding)\n",
        "```\n",
        "What is the effect of increasing the regularization rate (C) in logistic regression?\n",
        "a) The model becomes less prone to overfitting\n",
        "b) The model's training accuracy increases\n",
        "c) The model becomes more prone to overfitting\n",
        "d) The model's test accuracy increases\n",
        "\n",
        "Answer: c) The model becomes more prone to overfitting\n",
        "\n",
        "Explanation:\n",
        "Increasing the regularization rate (C) in logistic regression reduces the impact of regularization, making the model more prone to overfitting.\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Oew_Zo38USnp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quiz** (Check your understanding)\n",
        "```\n",
        "How does the regularization rate (C) affect the magnitude of the model coefficients in logistic regression?\n",
        "a) Higher C results in larger coefficient values\n",
        "b) Higher C results in smaller coefficient values\n",
        "c) C has no impact on the magnitude of the coefficients\n",
        "d) The effect of C on the coefficients depends on the dataset\n",
        "\n",
        "Answer: b) Higher C results in smaller coefficient values\n",
        "\n",
        "Explanation:\n",
        "higher values of the regularization rate (C) lead to smaller coefficient values.\n",
        "Because higher C increases the penalty for large coefficients during training\n",
        "Which pushes model to shrink the coefficient magnitudes to reduce the loss\n",
        "```"
      ],
      "metadata": {
        "id": "Qrye0M9YUcjz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logit/ Log odds\n"
      ],
      "metadata": {
        "id": "UDVPbjGQT1El"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quiz** (What do you think?)\n",
        "\n",
        "```\n",
        "The logistic regression algorithm estimates the parameters by maximizing the:\n",
        "a) Sum of squared errors\n",
        "b) Mean squared error\n",
        "c) Likelihood function\n",
        "d) Cross-entropy loss\n",
        "\n",
        "Ans: Likelihood function\n",
        "\n",
        "Explanation:\n",
        "The logistic regression algorithm estimates the parameters by maximizing the likelihood function. Or minimizing the negative likelihood.\n",
        "The likelihood function measures how well the chosen parameters fit the observed data\n",
        "```"
      ],
      "metadata": {
        "id": "pz1ASJceSnL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Log odds interpretation of logistic regression"
      ],
      "metadata": {
        "id": "zsXQM14SUBRX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1z-0qkx0h81U_iwb7fVeFQG0RVkpqyPGy' width=800>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "a0xeOqWdQ820"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1mruiW2aBWCEMjW74WtAC3_AQoeDZ4EdJ' width=800>\n",
        "\n"
      ],
      "metadata": {
        "id": "bf-fBZUeQ_9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Which concept of earlier is this similar to?\n",
        "\n",
        "Remember, $σ(p)$ also defined probability.\n",
        "\n",
        "So if we simplify our winning/losing as belonging to class 1/0, then $σ(p)$ here defines the probability of belonging to class 1 (winning class)"
      ],
      "metadata": {
        "id": "l_Mieok8UwK6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quiz** (What do you think?)\n",
        "\n",
        "```\n",
        "The logistic regression model predicts:\n",
        "a) Probabilities\n",
        "b) Class labels\n",
        "c) Continuous values\n",
        "d) Ordinal values\n",
        "\n",
        "Ans: a) Probabilities\n",
        "\n",
        "Explanation:\n",
        "Logistic regression predicts the probabilities of the target variable belonging to the positive class\n",
        "```"
      ],
      "metadata": {
        "id": "Wrpqn3nNS8Qn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1Xpm2xAc1oT95bAzZvRQPUobikSRR2Fgs' width=800>\n",
        "\n"
      ],
      "metadata": {
        "id": "P0QOJK4qRiV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1XWM57akV5CFtG8JypxDELnpNokU6nLco' width=800>"
      ],
      "metadata": {
        "id": "uW7oJ9DyRlv-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What does this mean geometrically?\n",
        "\n"
      ],
      "metadata": {
        "id": "HQJ1v0Tyjopp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=17CVyUuT9ZLlsqgWhsyKUChPP0o6Nlw33' width=800>"
      ],
      "metadata": {
        "id": "TmvYyUR4RouJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quiz** (Check your understanding)\n",
        "\n",
        "```\n",
        "If log(odds) is negative, which of the options hold true?\n",
        "\n",
        "a. 1-p > p\n",
        "b. p > 1-p\n",
        "c. p == 1-p\n",
        "\n",
        "Ans: a. 1-p > p\n",
        "\n",
        "Explanation: since odds = p/1-p, negative log value would mean p/1-p is <1, which would mean 1-p>p\n",
        "```"
      ],
      "metadata": {
        "id": "LQYWCoHHWXRK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1F7pWJ-_hmPbEe7LgaJhC9VESNrx0Y24x\n",
        "' width=800>"
      ],
      "metadata": {
        "id": "tGuGV4VxRto6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find the probability of the point lying, we simply apply exponential to both sides and solve for p, which would give:\n",
        "\n",
        "$p=\\frac{1}{1+e^{-z}}$\n",
        "\n",
        "Note: Sigmoid and Logit and just inverse of each other, and both can be used to build a logistic regression model"
      ],
      "metadata": {
        "id": "aBOq4_z2pJzI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quiz** (Check your understanding)\n",
        "\n",
        "```\n",
        "What is the range of log odds in logistic regression?\n",
        "a) (0, 1)\n",
        "b) (-∞, ∞)\n",
        "c) [0, 1]\n",
        "d) [0, ∞)\n",
        "\n",
        "Ans: b) (-∞, ∞)\n",
        "\n",
        "Explanation:\n",
        "The log odds in logistic regression can take any real value ranging from negative infinity to positive infinity.\n",
        "This is because it is the logarithm of the odds ratio, which is a continuous value.\n",
        "```"
      ],
      "metadata": {
        "id": "zH5-1-GmTKAj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quiz** (Check your understanding)\n",
        "\n",
        "```\n",
        "How are log odds transformed into probabilities in logistic regression?\n",
        "a) By applying the sigmoid function\n",
        "b) By taking the exponential function\n",
        "c) By dividing by the odds ratio\n",
        "d) By subtracting the intercept term\n",
        "\n",
        "Ans: a) By applying the sigmoid function\n",
        "\n",
        "Explanation:\n",
        "The sigmoid function maps the log odds to a value between 0 and 1\n",
        "```"
      ],
      "metadata": {
        "id": "UHyTqU0iTUOw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Impact of outliers"
      ],
      "metadata": {
        "id": "-xAtYk8zpO1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1aQk_WFojHob2thbycSBBC1hXx2cIL2Lh' width=800>\n",
        "\n"
      ],
      "metadata": {
        "id": "_JTcrJ7WR7ZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Case I: When the outlier lies on the correct side"
      ],
      "metadata": {
        "id": "kE0hLR1LxJWP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, $\\hat{y}=σ(z^i)$\n"
      ],
      "metadata": {
        "id": "5isiTDePxz2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/036/753/original/image_2023-06-14_052158593.png?1686700322\" height=500 width=600>"
      ],
      "metadata": {
        "id": "xqBbYquJyBgd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1iDeFLogS9rCNs1WiELMsFoRMIx_jRHZ8' width=800>\n",
        "\n"
      ],
      "metadata": {
        "id": "lwVx9PzSSPB8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the Loss is very less in this case:\n",
        "\n",
        "=> The impact of outlier is **very less**"
      ],
      "metadata": {
        "id": "PkxuXLvZLYGX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Case II: When the outlier is on the opposite/wrong side"
      ],
      "metadata": {
        "id": "tMCSU33-yxo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1SKv32h8SUGk4pbOuS6XQnCv20LMnUV6V' width=800>\n"
      ],
      "metadata": {
        "id": "tA9aeuNJSTaV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's say $z^i=-4.3$\n",
        "\n",
        "So $\\hat{y}$ becomes 0.01\n",
        "\n",
        "Therefore, L = $-log_e(0.01)$\n",
        "\n",
        "This comes out almost equal to 4.6, which is a very large value\n",
        "\n",
        "=> The impact of outlier will be **very high**"
      ],
      "metadata": {
        "id": "c-rNTFeZ05R9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thus the best thing is to find the outlier and remove them, so that we get accurate results"
      ],
      "metadata": {
        "id": "gnk3wKNI1a53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quiz** (Check your understanding)\n",
        "\n",
        "```\n",
        "How do outliers affect the classification boundaries in logistic regression?\n",
        "a) Outliers shift the classification boundaries closer to the outlier values\n",
        "b) Outliers have no effect on the classification boundaries\n",
        "c) Outliers widen the gap between the classification boundaries\n",
        "d) Outliers make the classification boundaries more sensitive to minor changes\n",
        "\n",
        "Answer: a) Outliers shift the classification boundaries closer to the outlier values\n",
        "```"
      ],
      "metadata": {
        "id": "1lORFoSNu5cq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-class classification"
      ],
      "metadata": {
        "id": "gaHfUqyd1oSI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Till now we have seen how to use logistic regression to classify between two classes\n",
        "\n",
        "But in real world there will be cases with many more classes\n",
        "\n",
        "#### How can we use logistic regression in cases with more than two output classes?"
      ],
      "metadata": {
        "id": "ZFD7VRYKD_8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1ZXmXc62oRRLsGOxNVvHi4GWITISWvL16' width=800>\n",
        "\n"
      ],
      "metadata": {
        "id": "ODVNdBSUS__u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1MSTuz_D9AJUZlHgDqMwQsBsyTLAE2gE7' width=800>\n",
        "\n"
      ],
      "metadata": {
        "id": "XbOhQ6yITE8a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train these models, we can't use the same dataset, since our data will have three classes."
      ],
      "metadata": {
        "id": "GIEbgDyNE0Ap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we will modify our data for the three models.\n",
        "\n",
        "Say for model 1, to check whether the input is orange or not,\n",
        "- Our output column will be modified by replacing the values with orange as 1, and rest values with 0\n",
        "\n",
        "We will do the same for the other two models"
      ],
      "metadata": {
        "id": "Z3VUT2sWFCjH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1xCJJoF5j0HJILD0xfhI6hA_1RqwoefHz' width=800>\n",
        "\n"
      ],
      "metadata": {
        "id": "PeBnKTPXTcTW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=15kHWLomnIvIkr6EmzB1EiDpAddlOQ-q2' width=800>\n",
        "\n"
      ],
      "metadata": {
        "id": "_XBa7YWbTyK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quiz** (Check your understanding)\n",
        "\n",
        "```\n",
        "We want to classify cars based on the 20 different brands of cars.\n",
        "How many logisitic Regression model will we need ?\n",
        "\n",
        "a. 10\n",
        "b. 20\n",
        "c. 21\n",
        "d. 19\n",
        "\n",
        "\n",
        "b. 20\n",
        "\n",
        "**Explanation**\n",
        "\n",
        "if we have yi = {1,2,3...K} in the  dataset, we have to generate K-binary classifier models.\n",
        "```"
      ],
      "metadata": {
        "id": "mAlj23QwVPmL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Now given an input point, how to predict which class it belongs to?"
      ],
      "metadata": {
        "id": "AmYCvtDUFVGY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1RTcgUwMq12FlqHJBH3l0jl91mbfCMQxv' width=800>\n",
        "\n"
      ],
      "metadata": {
        "id": "YKIUyNimT6kr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quiz** (Check your understanding)\n",
        "\n",
        "```\n",
        "For three models, the yhat values come to be:\n",
        "M1=0.34\n",
        "M2=0.28\n",
        "M3=0.35\n",
        "\n",
        "What would be the predicted output class by the classifier?\n",
        "\n",
        "a. M1\n",
        "b. M2\n",
        "c. M3\n",
        "d. None since no model has yhat>0.5\n",
        "\n",
        "Ans: c.M3\n",
        "\n",
        "Explanation: The model with the highest yhat value will be chosen irrespective of whether they are greater than the threshold or not\n",
        "```"
      ],
      "metadata": {
        "id": "2KVv6-S4W5uZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quiz** (Check your understanding)\n",
        "\n",
        "```\n",
        "What is the purpose of the one-vs-rest (OvR) strategy in multi-class logistic regression?\n",
        "a) To improve the interpretability of the model coefficients\n",
        "b) To handle imbalanced datasets in multi-class problems\n",
        "c) To reduce the complexity of the model\n",
        "d) To transform a multi-class problem into multiple binary classification problems\n",
        "Answer: d)To transform a multi-class problem into multiple binary classification problems\n",
        "\n",
        "Explanation:\n",
        "The one-vs-rest (OvR) strategy is used in multi-class logistic regression to transform the multi-class problem into multiple binary classification problems.\n",
        "We build n models for n classes where in each model we treat each class as the positive class and the rest of the classes as the negative class.\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "PZajWQVQTo2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see an implementation of the same using sklearn"
      ],
      "metadata": {
        "id": "3ddgQAxxGKDA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sklearn Code implementation for MultiClass Classification"
      ],
      "metadata": {
        "id": "7mGI1HEcX3xB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAAkds7oBik1"
      },
      "source": [
        "Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2aAoXVp-1bQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.inspection import DecisionBoundaryDisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x8zt9JRB03X"
      },
      "source": [
        "Creating some data with multiple classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQtc5UXb-1Of"
      },
      "outputs": [],
      "source": [
        "# dataset creation with 3 classes\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "X, y = make_classification(n_samples= 498,\n",
        "                           n_features= 2,\n",
        "                           n_classes = 3,\n",
        "                           n_redundant=0,\n",
        "                           n_clusters_per_class=1,\n",
        "                           random_state=5)\n",
        "y=y.reshape(len(y), 1)\n",
        "\n",
        "print(X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting the data"
      ],
      "metadata": {
        "id": "ZSLrSn-ndKho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X[:, 0], X[:, 1], c = y)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uE89OzPzYZa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the data into train validation and test set"
      ],
      "metadata": {
        "id": "cb7VVJvodMat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_tr_cv, X_test, y_tr_cv, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_tr_cv, y_tr_cv, test_size=0.25,random_state=4)\n",
        "X_train.shape"
      ],
      "metadata": {
        "id": "wx0egsivY3z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = y_train.reshape(-1) # making 1D vector"
      ],
      "metadata": {
        "id": "r2UF5si_6OgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mTCcWgve6yAf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "training the OneVsRest Logistic Regression model"
      ],
      "metadata": {
        "id": "Si9tkDF2dRRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(multi_class='auto')\n",
        "# fit model\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "PNmPrfHPY3L0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking the Accuracy of Training, validation and Test dataset"
      ],
      "metadata": {
        "id": "TnaFcUL8dXan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Training Accuracy:{model.score(X_train,y_train)}')\n",
        "print(f'Validation Accuracy :{model.score(X_val,y_val)}')\n",
        "print(f'Test Accuracy:{model.score(X_test,y_test)}')"
      ],
      "metadata": {
        "id": "JML2FkWgZ7ZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating Hyperplane of OVR LogisticRegression for the entire data"
      ],
      "metadata": {
        "id": "CH-yHlhMdw3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "-8lQncaFRX_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, ax = plt.subplots()\n",
        "DecisionBoundaryDisplay.from_estimator(model, X, response_method=\"predict\", cmap=plt.cm.Paired, ax=ax)\n",
        "plt.title(\"Decision surface of LogisticRegression\")\n",
        "plt.axis(\"tight\")\n",
        "\n",
        "# Plot also the training points\n",
        "colors = \"bry\"\n",
        "for i, color in zip(model.classes_, colors):\n",
        "        idx = np.where(y == i)\n",
        "        plt.scatter(\n",
        "            X[idx, 0], X[idx, 1], c=color, cmap=plt.cm.Paired, edgecolor=\"black\", s=20\n",
        "        )\n",
        "\n",
        "\n",
        "# Plot the three one-against-all classifiers\n",
        "xmin, xmax = plt.xlim()\n",
        "ymin, ymax = plt.ylim()\n",
        "coef = model.coef_\n",
        "intercept = model.intercept_\n",
        "\n",
        "def plot_hyperplane(c, color):\n",
        "        def line(x0):\n",
        "            return (-(x0 * coef[c, 0]) - intercept[c]) / coef[c, 1]\n",
        "\n",
        "        plt.plot([xmin, xmax], [line(xmin), line(xmax)], ls=\"--\", color=color)\n",
        "\n",
        "for i, color in zip(model.classes_, colors):\n",
        "        plot_hyperplane(i, color)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bMzyjx2bb3_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observe**\n",
        "\n",
        "We can see how One-vs-Rest Logistic Regression is able to classify Multi-class Classification data"
      ],
      "metadata": {
        "id": "OfcbDZYNd9UF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extra Quizzes**"
      ],
      "metadata": {
        "id": "qlCMfdn3UmQZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quiz** (What do you think?)\n",
        "```\n",
        "Which evaluation metric is commonly used to assess the performance of a logistic regression model?\n",
        "a) Mean squared error\n",
        "b) R-squared value\n",
        "c) Accuracy\n",
        "d) Root mean squared error\n",
        "\n",
        "Answer: c) Accuracy\n",
        "Explanation:\n",
        "Accuracy is a commonly used metric to check the performance of a logistic regression model.\n",
        "It measures the ratio of correctly predicted datapoints out of the total number of datapoints.\n",
        "```"
      ],
      "metadata": {
        "id": "vDwq6U2MUoj_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quiz** (Check your understanding)\n",
        "\n",
        "```\n",
        "Logistic regression assumes that the relationship between the independent variables and the log-odds of the dependent variable is:\n",
        "a) Exponential\n",
        "b) Quadratic\n",
        "c) Non-linear\n",
        "d) Linear\n",
        "Answer: d) Linear\n",
        "```"
      ],
      "metadata": {
        "id": "WpIUposuUwrC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quiz** (What do you think?)\n",
        "\n",
        "```\n",
        "How is the loss function typically defined in multi-class logistic regression?\n",
        "a) Cross-entropy loss\n",
        "b) Mean squared error (MSE)\n",
        "c) Mean absolute error (MAE)\n",
        "d) Hinge loss\n",
        "\n",
        "Answer: a) Cross-entropy loss\n",
        "```"
      ],
      "metadata": {
        "id": "uBap7_bgU5UM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HBYmf8pDMwvX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}