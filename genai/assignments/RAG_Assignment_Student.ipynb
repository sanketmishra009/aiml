{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ms3vnJY_Mni"
      },
      "source": [
        "# RAG with HuggingFace and Milvus - Student Notebook\n",
        "\n",
        "In this assignment, you will implement a complete RAG (Retrieval-Augmented Generation) pipeline using:\n",
        "- **Dataset**: HuggingFace Documentation (`m-ric/huggingface_doc`)\n",
        "- **Vector Store**: Milvus\n",
        "- **Embeddings**: BGE-small-en-v1.5\n",
        "- **LLM**: Microsoft Phi-3-mini-4k-instruct/\"Qwen/Qwen2-1.5B-Instruct\"\n",
        "- **Evaluation**: Opik (AnswerRelevance, Hallucination)\n",
        "\n",
        "## Instructions\n",
        "1. Read through each section carefully\n",
        "2. Complete the code in cells marked with `# TODO`\n",
        "3. Run all cells in order\n",
        "4. Verify your implementation with the evaluation cells\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##https://github.com/milvus-io/milvus"
      ],
      "metadata": {
        "id": "tvdmsptpGbVw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBlF01io_Mnj"
      },
      "source": [
        "## 1. Setup\n",
        "\n",
        "Install required dependencies and configure environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iq5jJcLv_Mnj"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q pymilvus sentence-transformers datasets transformers torch accelerate opik tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEugGiLC_Mnk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from typing import List, Dict, Tuple\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set your HuggingFace token for model access\n",
        "# You can get one at: https://huggingface.co/settings/tokens\n",
        "os.environ[\"HF_TOKEN\"] = \"hf_...\"  # Replace with your token\n",
        "\n",
        "# Opik configuration (optional - for generation evaluation)\n",
        "# Get your API key at: https://www.comet.com/\n",
        "os.environ[\"OPIK_API_KEY\"] = \"\"  # Replace with your Opik API key if available\n",
        "\n",
        "print(\"Environment configured!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pHDkDSt_Mnk"
      },
      "source": [
        "## 2. Data Loading\n",
        "\n",
        "Load the HuggingFace documentation dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfrrSuwy_Mnk"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the HuggingFace documentation dataset\n",
        "dataset = load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\n",
        "\n",
        "print(f\"Dataset loaded with {len(dataset)} documents\")\n",
        "print(f\"Columns: {dataset.column_names}\")\n",
        "print(f\"\\nSample document (first 500 chars):\")\n",
        "print(dataset[0][\"text\"][:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOoX5-1-_Mnk"
      },
      "outputs": [],
      "source": [
        "# Extract text and source information\n",
        "documents = []\n",
        "for item in dataset:\n",
        "    documents.append({\n",
        "        \"text\": item[\"text\"],\n",
        "        \"source\": item[\"source\"]\n",
        "    })\n",
        "\n",
        "print(f\"Extracted {len(documents)} documents\")\n",
        "\n",
        "# For this assignment, we'll use a subset to keep things manageable\n",
        "MAX_DOCS = 500\n",
        "documents = documents[:MAX_DOCS]\n",
        "print(f\"Using {len(documents)} documents for this assignment\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgDpZKnL_Mnk"
      },
      "source": [
        "## 3. Chunking\n",
        "\n",
        "Split documents into smaller chunks for better retrieval.\n",
        "\n",
        "### Your Task\n",
        "Implement the `chunk_document` function that:\n",
        "1. Takes a text string, chunk_size, and chunk_overlap as parameters\n",
        "2. Splits the text into overlapping chunks of the specified size\n",
        "3. Returns a list of chunk strings\n",
        "\n",
        "### Hints\n",
        "- Use a sliding window approach with step = chunk_size - chunk_overlap\n",
        "- Handle edge cases: empty text, text shorter than chunk_size\n",
        "- Make sure each chunk is non-empty before adding it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOXtgseg_Mnl"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# TODO: IMPLEMENT CHUNKING (15 points)\n",
        "# ============================================================\n",
        "\n",
        "def chunk_document(text: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> List[str]:\n",
        "    \"\"\"\n",
        "    Split a document into overlapping chunks of fixed size.\n",
        "\n",
        "    Args:\n",
        "        text: The document text to chunk\n",
        "        chunk_size: Maximum size of each chunk in characters\n",
        "        chunk_overlap: Number of overlapping characters between chunks\n",
        "\n",
        "    Returns:\n",
        "        List of text chunks\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "\n",
        "    # TODO: Implement fixed-size chunking with overlap\n",
        "    #\n",
        "    # Step 1: Handle edge cases (empty text, short text)\n",
        "    # Hint: If text is empty or shorter than chunk_size, return it as a single chunk\n",
        "\n",
        "    # Step 2: Calculate step size for sliding window\n",
        "    # Hint: step = chunk_size - chunk_overlap\n",
        "\n",
        "    # Step 3: Create chunks using a while loop\n",
        "    # Hint: Start at position 0, extract chunk_size characters,\n",
        "    #       then move forward by step size\n",
        "\n",
        "    # Step 4: Only add non-empty chunks (use .strip() to check)\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    pass\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def chunk_all_documents(documents: List[Dict], chunk_size: int = 1000, chunk_overlap: int = 200) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Chunk all documents and preserve metadata.\n",
        "\n",
        "    Args:\n",
        "        documents: List of document dicts with 'text' and 'source' keys\n",
        "        chunk_size: Maximum chunk size\n",
        "        chunk_overlap: Overlap between chunks\n",
        "\n",
        "    Returns:\n",
        "        List of chunk dicts with 'text', 'source', and 'chunk_id' keys\n",
        "    \"\"\"\n",
        "    all_chunks = []\n",
        "    chunk_id = 0\n",
        "\n",
        "    # TODO: Iterate through documents, chunk each one, and add metadata\n",
        "    #\n",
        "    # For each document:\n",
        "    #   1. Get text and source from document dict\n",
        "    #   2. Call chunk_document() to get chunks\n",
        "    #   3. For each chunk, create a dict with chunk_id, text, and source\n",
        "    #   4. Append to all_chunks and increment chunk_id\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    pass\n",
        "\n",
        "    return all_chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6D6kNKbW_Mnl"
      },
      "outputs": [],
      "source": [
        "# Test your chunking implementation\n",
        "test_text = \"A\" * 2500  # 2500 characters\n",
        "test_chunks = chunk_document(test_text, chunk_size=1000, chunk_overlap=200)\n",
        "\n",
        "print(f\"Test: 2500 char text with chunk_size=1000, overlap=200\")\n",
        "print(f\"Expected chunks: ~4\")\n",
        "print(f\"Your chunks: {len(test_chunks)}\")\n",
        "\n",
        "if len(test_chunks) >= 3 and len(test_chunks) <= 5:\n",
        "    print(\"✅ Chunking test passed!\")\n",
        "else:\n",
        "    print(\"❌ Check your chunking implementation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llntJu71_Mnl"
      },
      "outputs": [],
      "source": [
        "# Create chunks from all documents\n",
        "CHUNK_SIZE = 1000\n",
        "CHUNK_OVERLAP = 200\n",
        "\n",
        "chunks = chunk_all_documents(documents, CHUNK_SIZE, CHUNK_OVERLAP)\n",
        "\n",
        "print(f\"\\nCreated {len(chunks)} chunks from {len(documents)} documents\")\n",
        "print(f\"Average chunks per document: {len(chunks) / len(documents):.2f}\")\n",
        "\n",
        "# Show sample chunk\n",
        "if chunks:\n",
        "    print(f\"\\nSample chunk:\")\n",
        "    print(f\"  ID: {chunks[0]['chunk_id']}\")\n",
        "    print(f\"  Source: {chunks[0]['source']}\")\n",
        "    print(f\"  Text (first 200 chars): {chunks[0]['text'][:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_Tn_ML1_Mnl"
      },
      "source": [
        "## 4. Embeddings\n",
        "\n",
        "Generate vector embeddings for each chunk using BGE-small-en-v1.5.\n",
        "\n",
        "### Your Task\n",
        "Implement the `generate_embeddings` function that:\n",
        "1. Processes texts in batches for memory efficiency\n",
        "2. Uses the SentenceTransformer model to generate embeddings\n",
        "3. Returns embeddings as a list of lists (for Milvus compatibility)\n",
        "\n",
        "### Hints\n",
        "- Use `model.encode()` with `normalize_embeddings=True` for cosine similarity\n",
        "- Process in batches to avoid memory issues\n",
        "- Convert numpy arrays to lists using `.tolist()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lqh6daOW_Mnl"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load the embedding model\n",
        "EMBEDDING_MODEL = \"BAAI/bge-small-en-v1.5\" #Use any model of your choice from Sentence Transformers\n",
        "embedding_model = SentenceTransformer(EMBEDDING_MODEL)\n",
        "\n",
        "print(f\"Loaded embedding model: {EMBEDDING_MODEL}\")\n",
        "\n",
        "# Test embedding\n",
        "test_embedding = embedding_model.encode([\"This is a test\"], normalize_embeddings=True)\n",
        "EMBEDDING_DIM = len(test_embedding[0])\n",
        "print(f\"Embedding dimension: {EMBEDDING_DIM}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLJoa2-r_Mnm"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# TODO: IMPLEMENT EMBEDDING GENERATION (15 points)\n",
        "# ============================================================\n",
        "\n",
        "def generate_embeddings(texts: List[str], model: SentenceTransformer, batch_size: int = 32) -> List[List[float]]:\n",
        "    \"\"\"\n",
        "    Generate embeddings for a list of texts.\n",
        "\n",
        "    Args:\n",
        "        texts: List of text strings to embed\n",
        "        model: SentenceTransformer model\n",
        "        batch_size: Number of texts to process at once\n",
        "\n",
        "    Returns:\n",
        "        List of embedding vectors (as lists of floats)\n",
        "    \"\"\"\n",
        "    all_embeddings = []\n",
        "\n",
        "    # TODO: Generate embeddings in batches\n",
        "    #\n",
        "    # Step 1: Loop through texts in batches of size batch_size\n",
        "    # Hint: Use range(0, len(texts), batch_size) to get batch start indices\n",
        "\n",
        "    # Step 2: For each batch, call model.encode() with:\n",
        "    #   - The batch of texts\n",
        "    #   - normalize_embeddings=True (important for cosine similarity)\n",
        "    #   - show_progress_bar=False (we use tqdm at the outer level)\n",
        "\n",
        "    # Step 3: Convert to list format and extend all_embeddings\n",
        "    # Hint: Use .tolist() to convert numpy array to Python list\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    pass\n",
        "\n",
        "    return all_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cd58JLfI_Mnm"
      },
      "outputs": [],
      "source": [
        "# Test your embedding generation\n",
        "test_texts = [\"Hello world\", \"This is a test\", \"RAG is cool\"]\n",
        "test_embeddings = generate_embeddings(test_texts, embedding_model)\n",
        "\n",
        "print(f\"Generated {len(test_embeddings)} embeddings\")\n",
        "print(f\"Embedding dimension: {len(test_embeddings[0]) if test_embeddings else 0}\")\n",
        "\n",
        "if len(test_embeddings) == 3 and len(test_embeddings[0]) == 384:\n",
        "    print(\"✅ Embedding generation test passed!\")\n",
        "else:\n",
        "    print(\"❌ Check your embedding implementation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OreJHVF2_Mnm"
      },
      "outputs": [],
      "source": [
        "# Generate embeddings for all chunks\n",
        "chunk_texts = [chunk[\"text\"] for chunk in chunks]\n",
        "embeddings = generate_embeddings(chunk_texts, embedding_model)\n",
        "\n",
        "print(f\"\\nGenerated {len(embeddings)} embeddings\")\n",
        "if embeddings:\n",
        "    print(f\"Embedding dimension: {len(embeddings[0])}\")\n",
        "    print(f\"Sample embedding (first 10 values): {embeddings[0][:10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qpQmJzk_Mnm"
      },
      "source": [
        "## 5. Vector Store (Milvus)\n",
        "\n",
        "Store embeddings in Milvus for efficient similarity search.\n",
        "\n",
        "### Your Task\n",
        "1. Implement `setup_milvus_collection` to create a new collection\n",
        "2. Implement `insert_data_to_milvus` to insert chunks and embeddings\n",
        "\n",
        "### Hints\n",
        "- Use `client.has_collection()` to check if collection exists\n",
        "- Use `client.drop_collection()` to remove existing collection\n",
        "- Use `client.create_collection()` with dimension and metric_type parameters\n",
        "- Use `client.insert()` to add data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwIcljiX_Mnm"
      },
      "outputs": [],
      "source": [
        "from pymilvus import MilvusClient\n",
        "\n",
        "# Initialize Milvus client (uses Milvus Lite - stores data locally)\n",
        "MILVUS_DB_PATH = \"./hf_docs_milvus.db\"\n",
        "milvus_client = MilvusClient(uri=MILVUS_DB_PATH)\n",
        "\n",
        "COLLECTION_NAME = \"hf_documentation\"\n",
        "\n",
        "print(f\"Milvus client initialized with database: {MILVUS_DB_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMAHuGsp_Mnm"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# TODO: IMPLEMENT MILVUS COLLECTION SETUP (10 points)\n",
        "# ============================================================\n",
        "\n",
        "def setup_milvus_collection(client: MilvusClient, collection_name: str, embedding_dim: int):\n",
        "    \"\"\"\n",
        "    Create a Milvus collection for storing document embeddings.\n",
        "\n",
        "    Args:\n",
        "        client: MilvusClient instance\n",
        "        collection_name: Name of the collection to create\n",
        "        embedding_dim: Dimension of the embedding vectors\n",
        "    \"\"\"\n",
        "    # TODO: Create a Milvus collection\n",
        "    #\n",
        "    # Step 1: Check if collection already exists using client.has_collection()\n",
        "    # Step 2: If exists, drop it using client.drop_collection()\n",
        "    # Step 3: Create new collection using client.create_collection() with:\n",
        "    #   - collection_name: the name parameter\n",
        "    #   - dimension: embedding_dim parameter\n",
        "    #   - metric_type: \"IP\" (Inner Product for cosine similarity)\n",
        "    #   - consistency_level: \"Strong\"\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    pass\n",
        "\n",
        "    print(f\"Created collection: {collection_name} with dimension {embedding_dim}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtpqSzLd_Mnm"
      },
      "outputs": [],
      "source": [
        "# Setup the collection\n",
        "setup_milvus_collection(milvus_client, COLLECTION_NAME, EMBEDDING_DIM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-a17gp9U_Mnm"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# TODO: IMPLEMENT DATA INSERTION (10 points)\n",
        "# ============================================================\n",
        "\n",
        "def insert_data_to_milvus(\n",
        "    client: MilvusClient,\n",
        "    collection_name: str,\n",
        "    chunks: List[Dict],\n",
        "    embeddings: List[List[float]],\n",
        "    batch_size: int = 100\n",
        "):\n",
        "    \"\"\"\n",
        "    Insert document chunks and embeddings into Milvus.\n",
        "\n",
        "    Args:\n",
        "        client: MilvusClient instance\n",
        "        collection_name: Name of the collection\n",
        "        chunks: List of chunk dictionaries with text and metadata\n",
        "        embeddings: List of embedding vectors\n",
        "        batch_size: Number of records to insert at once\n",
        "\n",
        "    Returns:\n",
        "        Total number of inserted records\n",
        "    \"\"\"\n",
        "    total_inserted = 0\n",
        "\n",
        "    # TODO: Insert data into Milvus\n",
        "    #\n",
        "    # Step 1: Prepare data as a list of dictionaries, where each dict has:\n",
        "    #   - \"id\": chunk[\"chunk_id\"]\n",
        "    #   - \"vector\": the corresponding embedding\n",
        "    #   - \"text\": chunk[\"text\"]\n",
        "    #   - \"source\": chunk[\"source\"]\n",
        "\n",
        "    # Step 2: Insert in batches using client.insert()\n",
        "    # Hint: Loop through data in batches and call:\n",
        "    #   result = client.insert(collection_name=collection_name, data=batch)\n",
        "    #   total_inserted += result[\"insert_count\"]\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    pass\n",
        "\n",
        "    return total_inserted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTzQZmUl_Mnm"
      },
      "outputs": [],
      "source": [
        "# Insert data into Milvus\n",
        "inserted_count = insert_data_to_milvus(milvus_client, COLLECTION_NAME, chunks, embeddings)\n",
        "\n",
        "print(f\"\\nInserted {inserted_count} records into Milvus\")\n",
        "\n",
        "if inserted_count == len(chunks):\n",
        "    print(\"✅ All chunks inserted successfully!\")\n",
        "else:\n",
        "    print(\"❌ Not all chunks were inserted. Check your implementation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1isFIiD_Mnm"
      },
      "source": [
        "## 6. Retrieval\n",
        "\n",
        "Implement semantic search to retrieve relevant documents for a query.\n",
        "\n",
        "### Your Task\n",
        "Implement the `retrieve_documents` function that:\n",
        "1. Generates an embedding for the query\n",
        "2. Searches Milvus for similar vectors\n",
        "3. Returns the top-K most relevant documents\n",
        "\n",
        "### Hints\n",
        "- Use `embedding_model.encode()` to embed the query\n",
        "- Use `client.search()` to find similar vectors\n",
        "- Extract text and source from the search results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfwXdwGO_Mnm"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# TODO: IMPLEMENT RETRIEVAL (25 points)\n",
        "# ============================================================\n",
        "\n",
        "def retrieve_documents(\n",
        "    query: str,\n",
        "    client: MilvusClient,\n",
        "    collection_name: str,\n",
        "    embedding_model: SentenceTransformer,\n",
        "    top_k: int = 5\n",
        ") -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Retrieve the most relevant documents for a query.\n",
        "\n",
        "    Args:\n",
        "        query: The search query\n",
        "        client: MilvusClient instance\n",
        "        collection_name: Name of the collection to search\n",
        "        embedding_model: Model to generate query embedding\n",
        "        top_k: Number of results to return\n",
        "\n",
        "    Returns:\n",
        "        List of dictionaries with 'text', 'source', and 'score' keys\n",
        "    \"\"\"\n",
        "    # TODO: Implement semantic search\n",
        "    #\n",
        "    # Step 1: Generate embedding for the query\n",
        "    # Hint: Use embedding_model.encode([query], normalize_embeddings=True)\n",
        "    #       Then convert to list: .tolist()[0]\n",
        "\n",
        "    # Step 2: Search in Milvus using client.search()\n",
        "    # Required parameters:\n",
        "    #   - collection_name: collection_name\n",
        "    #   - data: [query_embedding] (list containing the embedding)\n",
        "    #   - limit: top_k\n",
        "    #   - search_params: {\"metric_type\": \"IP\", \"params\": {}}\n",
        "    #   - output_fields: [\"text\", \"source\"]\n",
        "\n",
        "    # Step 3: Format results as list of dicts\n",
        "    # Each dict should have:\n",
        "    #   - \"text\": result[\"entity\"][\"text\"]\n",
        "    #   - \"source\": result[\"entity\"][\"source\"]\n",
        "    #   - \"score\": result[\"distance\"]\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    retrieved_docs = []\n",
        "\n",
        "    return retrieved_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Es39BSL_Mnn"
      },
      "outputs": [],
      "source": [
        "# Test retrieval\n",
        "test_query = \"How do I fine-tune a transformer model?\"\n",
        "\n",
        "retrieved = retrieve_documents(\n",
        "    query=test_query,\n",
        "    client=milvus_client,\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    embedding_model=embedding_model,\n",
        "    top_k=3\n",
        ")\n",
        "\n",
        "print(f\"Query: {test_query}\")\n",
        "print(f\"\\nRetrieved {len(retrieved)} documents:\")\n",
        "for i, doc in enumerate(retrieved):\n",
        "    print(f\"\\n--- Document {i+1} (Score: {doc.get('score', 'N/A')}) ---\")\n",
        "    print(f\"Source: {doc.get('source', 'N/A')}\")\n",
        "    print(f\"Text: {doc.get('text', 'N/A')[:300]}...\")\n",
        "\n",
        "if len(retrieved) == 3 and all('text' in d for d in retrieved):\n",
        "    print(\"\\n✅ Retrieval test passed!\")\n",
        "else:\n",
        "    print(\"\\n❌ Check your retrieval implementation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUm-anJf_Mnn"
      },
      "source": [
        "## 7. Generation\n",
        "\n",
        "Generate answers using Microsoft Phi-3-mini-4k-instruct/Qwen.\n",
        "\n",
        "### Your Task\n",
        "Implement the `generate_answer` function that:\n",
        "1. Combines retrieved documents into a context string\n",
        "2. Formats the prompt using the provided template\n",
        "3. Generates an answer using the language model\n",
        "4. Returns a structured result dictionary\n",
        "\n",
        "### Hints\n",
        "- Join document texts with newlines to create context\n",
        "- Use the PROMPT_TEMPLATE.format() to fill in context and question\n",
        "- Call the generator pipeline with appropriate parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### https://huggingface.co/microsoft/Phi-3-mini-4k-instruct"
      ],
      "metadata": {
        "id": "hlDeWGZ9JDfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### https://huggingface.co/microsoft/Phi-3.5-mini-instruct"
      ],
      "metadata": {
        "id": "B8w3YXOaJJ4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### https://huggingface.co/Qwen/Qwen2-1.5B-Instruct"
      ],
      "metadata": {
        "id": "U-Z9rpiGJPYW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c0ieMxESJRZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FEEL FREE TO USE A PROPRIETARY MODEL LIKE OPENAI, CLAUDE"
      ],
      "metadata": {
        "id": "YBamzDbZKOpK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "0o3E_QXR_Mnn",
        "outputId": "df58ac56-efc6-4c8f-c1e4-6e6f7d61696c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-373466297.py, line 5)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-373466297.py\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    LLM_MODEL = #SPECIFY THE MODEL FROM HUGGINGFACE\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "\n",
        "# Load the language model\n",
        "LLM_MODEL = #SPECIFY THE MODEL FROM HUGGINGFACE\n",
        "\n",
        "print(f\"Loading model: {LLM_MODEL}\")\n",
        "print(\"This may take a few minutes...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    LLM_MODEL,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Create text generation pipeline\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "print(f\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MODIFY THIS TO SUIT YOUR MODEL"
      ],
      "metadata": {
        "id": "Oq4ek9pfAS2Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OCUmNvk_Mnn"
      },
      "outputs": [],
      "source": [
        "# Prompt template for RAG (YOU ARE FREE TO MODIFY)\n",
        "PROMPT_TEMPLATE = \"\"\"Use the following pieces of information enclosed in <context> tags to provide an answer to the question enclosed in <question> tags.\n",
        "If the context doesn't contain enough information to answer the question, say \"I don't have enough information to answer this question.\"\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "<question>\n",
        "{question}\n",
        "</question>\n",
        "\n",
        "Answer:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ct-0KSE_Mnn"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# TODO: IMPLEMENT GENERATION (25 points)\n",
        "# ============================================================\n",
        "\n",
        "def generate_answer(\n",
        "    query: str,\n",
        "    retrieved_docs: List[Dict],\n",
        "    generator: pipeline,\n",
        "    max_new_tokens: int = 256\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Generate an answer using retrieved documents as context.\n",
        "\n",
        "    Args:\n",
        "        query: The user's question\n",
        "        retrieved_docs: List of retrieved document dictionaries\n",
        "        generator: HuggingFace text generation pipeline\n",
        "        max_new_tokens: Maximum tokens to generate\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with 'answer', 'context', 'query', and 'retrieved_docs'\n",
        "    \"\"\"\n",
        "    # TODO: Generate an answer using the RAG pattern\n",
        "    #\n",
        "    # Step 1: Combine retrieved documents into context\n",
        "    # Hint: Join doc[\"text\"] for each doc with \"\\n\\n\" separator\n",
        "\n",
        "    # Step 2: Format the prompt using PROMPT_TEMPLATE\n",
        "    # Hint: prompt = PROMPT_TEMPLATE.format(context=context, question=query)\n",
        "\n",
        "    # Step 3: Generate response using the generator pipeline\n",
        "    # Call generator() with:\n",
        "    #   - prompt (first argument)\n",
        "    #   - max_new_tokens=max_new_tokens\n",
        "    #   - do_sample=True\n",
        "    #   - temperature=0.7\n",
        "    #   - top_p=0.9\n",
        "    #   - return_full_text=False\n",
        "\n",
        "    # Step 4: Extract the generated text\n",
        "    # Hint: outputs[0][\"generated_text\"].strip()\n",
        "\n",
        "    # Step 5: Return result dictionary\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    context = \"\"\n",
        "    answer = \"\"\n",
        "\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"answer\": answer,\n",
        "        \"context\": context,\n",
        "        \"retrieved_docs\": retrieved_docs\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9pr48cM_Mnn"
      },
      "outputs": [],
      "source": [
        "# Test generation\n",
        "test_query = \"How do I fine-tune a transformer model?\"\n",
        "\n",
        "# Retrieve relevant documents\n",
        "retrieved = retrieve_documents(\n",
        "    query=test_query,\n",
        "    client=milvus_client,\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    embedding_model=embedding_model,\n",
        "    top_k=3\n",
        ")\n",
        "\n",
        "# Generate answer\n",
        "result = generate_answer(\n",
        "    query=test_query,\n",
        "    retrieved_docs=retrieved,\n",
        "    generator=generator\n",
        ")\n",
        "\n",
        "print(f\"Question: {result['query']}\")\n",
        "print(f\"\\nAnswer: {result['answer']}\")\n",
        "\n",
        "if result['answer'] and len(result['answer']) > 10:\n",
        "    print(\"\\n✅ Generation test passed!\")\n",
        "else:\n",
        "    print(\"\\n❌ Check your generation implementation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KveAEzm_Mnn"
      },
      "outputs": [],
      "source": [
        "# Complete RAG pipeline function (DO NOT MODIFY)\n",
        "\n",
        "def rag_query(\n",
        "    query: str,\n",
        "    client: MilvusClient,\n",
        "    collection_name: str,\n",
        "    embedding_model: SentenceTransformer,\n",
        "    generator: pipeline,\n",
        "    top_k: int = 5,\n",
        "    max_new_tokens: int = 256\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Complete RAG pipeline: retrieve then generate.\n",
        "    \"\"\"\n",
        "    # Retrieve\n",
        "    retrieved_docs = retrieve_documents(\n",
        "        query=query,\n",
        "        client=client,\n",
        "        collection_name=collection_name,\n",
        "        embedding_model=embedding_model,\n",
        "        top_k=top_k\n",
        "    )\n",
        "\n",
        "    # Generate\n",
        "    result = generate_answer(\n",
        "        query=query,\n",
        "        retrieved_docs=retrieved_docs,\n",
        "        generator=generator,\n",
        "        max_new_tokens=max_new_tokens\n",
        "    )\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDRlGGxQ_Mnn"
      },
      "outputs": [],
      "source": [
        "# Test complete pipeline with multiple queries\n",
        "test_queries = [\n",
        "    \"What is the Trainer class in transformers?\",\n",
        "    \"How do I load a dataset from HuggingFace?\",\n",
        "    \"What is Gradio used for?\"\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    result = rag_query(\n",
        "        query=query,\n",
        "        client=milvus_client,\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        embedding_model=embedding_model,\n",
        "        generator=generator,\n",
        "        top_k=3\n",
        "    )\n",
        "    print(f\"Q: {result['query']}\")\n",
        "    print(f\"A: {result['answer']}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}