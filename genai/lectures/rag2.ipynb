{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78627832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m113.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m385.2/385.2 kB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m114.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m115.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.5/17.5 MB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m117.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m128.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m340.3/340.3 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m96.8/96.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m329.1/329.1 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
      "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
      "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
      "google-adk 1.23.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\n",
      "google-adk 1.23.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain langchain-community langchain-huggingface chromadb ragatouille chonkie transformers accelerate bitsandbytes sentencepiece safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7c0e1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Feb  8 06:43:07 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   37C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "284f8d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b127a3d4de45a9af603e087aee7166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82dd2bc01f264b688367c271ca32db1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "huggingface_doc.csv:   0%|          | 0.00/22.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f566eac3af4455396af5b29479344ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2647 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2647\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset:\n",
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"m-ric/huggingface_doc\")\n",
    "print(f\"Number of documents: {len(data['train'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffce5a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text ,  Create an Endpoint\n",
      "\n",
      "After your first login, you will be directed to the [Endpoint creation page](https://ui.endpoints.huggingface.co/new). As an example, this guide will go through the steps to deploy [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) for text classification. \n",
      "\n",
      "## 1. Enter the Hugging Face Repository ID and your desired endpoint name:\n",
      "\n",
      "<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-docu...\n",
      "source. huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/create_endpoint.mdx\n"
     ]
    }
   ],
   "source": [
    "# print sample document:\n",
    "doc = data[\"train\"][0]\n",
    "\n",
    "for key, val in doc.items():\n",
    "    if isinstance(val, str) and len(val) > 500:\n",
    "        print(f\"{key} , {val[:500]}...\")\n",
    "    else:\n",
    "        print(f\"{key}. {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86523e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0296288fd1ac420a81944b77491a49c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of chunk-lists: 2647\n"
     ]
    }
   ],
   "source": [
    "# convert text to chunks:\n",
    "from chonkie import TokenChunker\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_pretrained(\"gpt2\")\n",
    "chunker = TokenChunker(tokenizer=tokenizer, chunk_size=256, chunk_overlap=32)\n",
    "\n",
    "chunks = []\n",
    "for doc in data[\"train\"]:\n",
    "    chunk = chunker.chunk(doc[\"text\"])\n",
    "    chunks.append(chunk)\n",
    "\n",
    "print(f\"Total # of chunk-lists: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba748827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of chunks to embed:2647\n"
     ]
    }
   ],
   "source": [
    "ctexts = [chunk[0].text for chunk in chunks]\n",
    "print(f\"# of chunks to embed:{len(ctexts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d61c09e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ddc75e645c49b7a41e3e521314ec7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d358be8f0d74723ae2ecd78f1b33a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcdfadf78867493e99b5fd195ac49c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df54e1b49c940ae80960f44bc79dabd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba277306fee4ef59482c4ad7dd37da4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6d87f3f8d841f7b710919f303a4c12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1796167bb1614dfd9870de805fdbd0ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea38603de6834f589eb345bb1890e190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e46d94be304c069ab0fc12c1b894e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0fcd9ad789949ee86b0fad7ef8778b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40bbe2022194e059da2aa08f7336478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded!(bi-encoder that processes queries and documents together!)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model=\"BAAI/bge-base-en-v1.5\")\n",
    "print(\n",
    "    \"Embedding model loaded!(bi-encoder that processes queries and documents together!)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05e647c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 2647 documents for indexing.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = []\n",
    "for i, text in enumerate(ctexts):\n",
    "    documents.append(\n",
    "        Document(\n",
    "            page_content=text, metadata={\"chunk_id\": i, \"source\": \"huggingface_doc\"}\n",
    "        )\n",
    "    )\n",
    "\n",
    "docs = filter_complex_metadata(documents)\n",
    "print(f\"Prepared {len(docs)} documents for indexing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "662e3f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created with HNSW indexing\n",
      "Total documents indexed: 2647\n"
     ]
    }
   ],
   "source": [
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"huggingface_docs_advanced\",\n",
    ")\n",
    "\n",
    "print(\"Vector store created with HNSW indexing\")\n",
    "print(f\"Total documents indexed: {vectorstore._collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e727846c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever configured:\n",
      "  - Search type: similarity (cosine distance)\n",
      "  - Initial candidates: 20\n",
      "  - Algorithm: HNSW (logarithmic search time)\n"
     ]
    }
   ],
   "source": [
    "# create retriver - bi-encoder for fast retrival!\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 20})\n",
    "\n",
    "print(\"Retriever configured:\")\n",
    "print(\"  - Search type: similarity (cosine distance)\")\n",
    "print(\"  - Initial candidates: 20\")\n",
    "print(\"  - Algorithm: HNSW (logarithmic search time)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cf89d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'How do I load a pretrained model?'\n",
      "Search time with HNSW: 57.70 ms\n",
      "Documents retrieved: 20\n",
      "\n",
      "================================================================================\n",
      "Top 3 Results:\n",
      "================================================================================\n",
      "\n",
      "Result 1:\n",
      " Quickstart\n",
      "\n",
      "This quickstart is intended for developers who are ready to dive into the code and see an example of how to integrate `timm` into their model training workflow.\n",
      "\n",
      "First, you'll need to ins...\n",
      "Source: huggingface_doc, Chunk ID: 1474\n",
      "\n",
      "Result 2:\n",
      "FrameworkSwitchCourse {fw} />\n",
      "\n",
      "# Fine-tuning a model with the Trainer API[[fine-tuning-a-model-with-the-trainer-api]]\n",
      "\n",
      "<CourseFloatingBanner chapter={3}\n",
      "  classNames=\"absolute z-10 right-0 top-0\"\n",
      "  no...\n",
      "Source: huggingface_doc, Chunk ID: 2398\n",
      "\n",
      "Result 3:\n",
      "FrameworkSwitchCourse {fw} />\n",
      "\n",
      "# Introduction[[introduction]]\n",
      "\n",
      "<CourseFloatingBanner\n",
      "    chapter={3}\n",
      "    classNames=\"absolute z-10 right-0 top-0\"\n",
      "/>\n",
      "\n",
      "In [Chapter 2](/course/chapter2) we explored how t...\n",
      "Source: huggingface_doc, Chunk ID: 2215\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Test query\n",
    "test_query = \"How do I load a pretrained model?\"\n",
    "\n",
    "# Measure search time with HNSW\n",
    "start_time = time.time()\n",
    "results = retriever.invoke(test_query)\n",
    "search_time = time.time() - start_time\n",
    "\n",
    "print(f\"Query: '{test_query}'\")\n",
    "print(f\"Search time with HNSW: {search_time*1000:.2f} ms\")\n",
    "print(f\"Documents retrieved: {len(results)}\\n\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Top 3 Results:\")\n",
    "print(\"=\" * 80)\n",
    "for i, doc in enumerate(results[:3], 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"{doc.page_content[:200]}...\")\n",
    "    print(f\"Source: {doc.metadata['source']}, Chunk ID: {doc.metadata['chunk_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e81e35",
   "metadata": {},
   "source": [
    "## HYDE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44b900af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83899a59b7e446f3b041604686f80156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "840fdea3b66c4570a9532670bdef1522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a99cd7f4e445339dba028e265a6603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cac0d653ac5c4f119995cec265119412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d637c9c3234bc0bc7bc59ffdd929d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f0fee8ad7ce4d47b1cf5df42ee9d1f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a208ecaf78744062b415e68f649f06e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f73055bcd944fea8d1d5884da046331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f5308d458844657a7a129dea4188741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8628d2a58c44f1a841b950515a4ad4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1efa2a8a97c945ce98b747d6357c0906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f781e1daf9d247da86a3317e8dc1242d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d11586639f4a459c9695a59ebb1a3327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 3584)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load qwen2.5-7b-instruct:\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "mid = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(mid, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    mid, quantization_config=bnb_config, device_map=\"auto\", trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ab82036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "untokenized chat template:\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Write a detailed, factual answer.\n",
      "Do not mention uncertainty.\n",
      "\n",
      "Question:\n",
      "What is vector similarity search in information retrieval?\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "tokenized chat template:\n",
      "tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
      "            553,  54364,  14817,     13,   1446,    525,    264,  10950,  17847,\n",
      "             13, 151645,    198, 151644,    872,    198,   7985,    264,  11682,\n",
      "             11,  59901,   4226,    624,   5404,    537,   6286,  26826,    382,\n",
      "          14582,    510,   3838,    374,   4621,  37623,   2711,    304,   1995,\n",
      "          56370,   5267, 151645,    198, 151644,  77091,    198]],\n",
      "       device='cuda:0')\n",
      "Generated text:system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
      "user\n",
      "Write a detailed, factual answer.\n",
      "Do not mention uncertainty.\n",
      "\n",
      "Question:\n",
      "What is vector similarity search in information retrieval?\n",
      "\n",
      "assistant\n",
      "Vector similarity search in information retrieval is a technique used to find items in a dataset that are similar to a given query item, where both the query and the dataset items are represented as vectors in a multi-dimensional space. This method is widely used in various applications such as recommendation systems, image retrieval, and natural language processing.\n",
      "\n",
      "In this context, a vector represents an item or document, with each dimension corresponding to a feature or attribute of the item. For example, in text data, each dimension might represent the frequency of a particular word, or in images, each dimension could represent pixel intensity values. The similarity between two vectors is often measured using distance metrics such as cosine similarity, Euclidean distance, or Manhattan distance.\n",
      "\n",
      "The process of vector similarity search involves the following steps:\n",
      "\n",
      "1. **Representation**: Convert the items or documents into numerical vectors. This can be done through techniques like term frequency-inverse document frequency (TF-IDF), word embeddings (e.g., Word2Vec, GloVe), or deep learning models that output vector representations.\n",
      "\n",
      "2. **Query Representation**: Represent the query in the same vector space as the dataset. This ensures that the similarity measure is meaningful across the entire dataset.\n",
      "\n",
      "3. **Similarity Calculation**: Compute the similarity between the query vector and all the vectors in the dataset. Common methods include calculating the cosine similarity, which measures the cosine of the angle between two vectors, or using other distance metrics depending on the application.\n",
      "\n",
      "4. **Search and Ranking**: Identify the top-k\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Write a detailed, factual answer.\n",
    "Do not mention uncertainty.\n",
    "\n",
    "Question:\n",
    "What is vector similarity search in information retrieval?\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False\n",
    ")\n",
    "print(f\"untokenized chat template:\\n{inputs}\")\n",
    "inputs = tokenizer(inputs, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "print(f\"tokenized chat template:\\n{inputs}\")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs, max_new_tokens=300, temperature=0.2, top_p=0.9, do_sample=True\n",
    "    )\n",
    "print(f\"Generated text:{tokenizer.decode(outputs[0] , skip_special_tokens= True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0eb753d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result 0:\n",
      "ext embeddings and semantic search. In this video weâ€™ll explore how Transformer models represent text as embedding vectors and how these vectors can be used to find similar documents in a corpus. Text...\n",
      "Source: huggingface_doc, Chunk ID: 1314\n",
      "\n",
      "Result 1:\n",
      " Search index\n",
      "\n",
      "[FAISS](https://github.com/facebookresearch/faiss) and [Elasticsearch](https://www.elastic.co/elasticsearch/) enables searching for examples in a dataset. This can be useful when you wa...\n",
      "Source: huggingface_doc, Chunk ID: 1172\n",
      "\n",
      "Result 2:\n",
      "--\n",
      "title: Image Similarity with Hugging Face Datasets and Transformers\n",
      "thumbnail: /blog/assets/image_similarity/thumbnail.png\n",
      "authors:\n",
      "- user: sayakpaul\n",
      "---\n",
      "\n",
      "# Image Similarity with Hugging Face Datas...\n",
      "Source: huggingface_doc, Chunk ID: 2226\n",
      "\n",
      "Result 3:\n",
      "--\n",
      "title: \"Retrieval Augmented Generation with Huggingface Transformers and Ray\"\n",
      "thumbnail: /blog/assets/12_ray_rag/ray_arch_updated.png\n",
      "authors:\n",
      "- user: ray-project\n",
      "  guest: true\n",
      "---\n",
      "\n",
      "# Retrieval Aug...\n",
      "Source: huggingface_doc, Chunk ID: 305\n",
      "\n",
      "Result 4:\n",
      "--\n",
      "title: \"Image search with ðŸ¤— datasets\"\n",
      "thumbnail: /blog/assets/54_image_search_datasets/spaces_image_search.jpg \n",
      "authors:\n",
      "- user: davanstrien\n",
      "  guest: true\n",
      "---\n",
      "\n",
      "# Image search with ðŸ¤— datasets  \n",
      "\n",
      "\n",
      "<a...\n",
      "Source: huggingface_doc, Chunk ID: 414\n",
      "\n",
      "Result 5:\n",
      "--\n",
      "title: 'Getting Started With Embeddings'\n",
      "thumbnail: /blog/assets/80_getting_started_with_embeddings/thumbnail.png\n",
      "authors:\n",
      "- user: espejelomar\n",
      "---\n",
      "\n",
      "# Getting Started With Embeddings\n",
      "\n",
      "\n",
      "Check out thi...\n",
      "Source: huggingface_doc, Chunk ID: 547\n",
      "\n",
      "Result 6:\n",
      "--\n",
      "title: Guiding Text Generation with Constrained Beam Search in ðŸ¤— Transformers\n",
      "thumbnail: /blog/assets/53_constrained_beam_search/thumbnail.png\n",
      "authors:\n",
      "- user: cwkeam\n",
      "  guest: true\n",
      "---\n",
      "\n",
      "# Guiding T...\n",
      "Source: huggingface_doc, Chunk ID: 160\n",
      "\n",
      "Result 7:\n",
      " Search\n",
      "\n",
      "You can now easily search anything on the Hub with **Full-text search**. We index model cards, dataset cards, and Spaces app.py files.\n",
      "\n",
      "Go directly to https://huggingface.co/search or, using ...\n",
      "Source: huggingface_doc, Chunk ID: 384\n",
      "\n",
      "Result 8:\n",
      "--\n",
      "title: \"Supercharged Searching on the ðŸ¤— Hub\"\n",
      "thumbnail: /blog/assets/48_hubsearch/thumbnail.png\n",
      "authors:\n",
      "- user: muellerzr\n",
      "---\n",
      "\n",
      "# Supercharged Searching on the Hugging Face Hub\n",
      "\n",
      "\n",
      "<a target=\"_blank\"...\n",
      "Source: huggingface_doc, Chunk ID: 1617\n",
      "\n",
      "Result 9:\n",
      "--\n",
      "title: \"Generating Human-level Text with Contrastive Search in Transformers ðŸ¤—\"\n",
      "thumbnail: /blog/assets/115_introducing_contrastive_search/thumbnail.png\n",
      "authors:\n",
      "- user: GMFTBY\n",
      "---\n",
      "\n",
      "# Generating Hum...\n",
      "Source: huggingface_doc, Chunk ID: 91\n",
      "\n",
      "Result 10:\n",
      "--\n",
      "title: \"Accelerating Document AI\" \n",
      "thumbnail: /blog/assets/112_document-ai/thumbnail.png\n",
      "authors:\n",
      "- user: rajistics\n",
      "- user: nielsr\n",
      "- user: florentgbelidji\n",
      "- user: nbroad\n",
      "---\n",
      "\n",
      "# Accelerating Documen...\n",
      "Source: huggingface_doc, Chunk ID: 144\n",
      "\n",
      "Result 11:\n",
      " Using Sentence Transformers at Hugging Face\n",
      "\n",
      "`sentence-transformers` is a library that provides easy methods to compute embeddings (dense vector representations) for sentences, paragraphs and images....\n",
      "Source: huggingface_doc, Chunk ID: 2046\n",
      "\n",
      "Result 12:\n",
      "--\n",
      "title: Hyperparameter Search with Transformers and Ray Tune\n",
      "thumbnail: /blog/assets/06_ray_tune/ray-hf.jpg\n",
      "authors:\n",
      "- user: ray-project\n",
      "  guest: true\n",
      "---\n",
      "\n",
      "# Hyperparameter Search with Transformers ...\n",
      "Source: huggingface_doc, Chunk ID: 2396\n",
      "\n",
      "Result 13:\n",
      " Search text in a dataset\n",
      "\n",
      "Datasets Server provides a `/search` endpoint for searching words in a dataset.\n",
      "\n",
      "<Tip warning={true}>\n",
      "  Currently, only <a href=\"./parquet\">datasets with Parquet exports</a>...\n",
      "Source: huggingface_doc, Chunk ID: 1690\n",
      "\n",
      "Result 14:\n",
      "--\n",
      "title: \"Train a Sentence Embedding Model with 1B Training Pairs\"\n",
      "authors:\n",
      "- user: asi\n",
      "  guest: true\n",
      "---\n",
      "\n",
      "# Train a Sentence Embedding Model with 1 Billion Training Pairs\n",
      "\n",
      "\n",
      "**Sentence embedding** is...\n",
      "Source: huggingface_doc, Chunk ID: 518\n",
      "\n",
      "Result 15:\n",
      "--\n",
      "title: \"Introduction to Graph Machine Learning\" \n",
      "thumbnail: /blog/assets/125_intro-to-graphml/thumbnail.png\n",
      "authors:\n",
      "- user: clefourrier\n",
      "---\n",
      "\n",
      "# Introduction to Graph Machine Learning\n",
      "\n",
      "\n",
      "In this blog...\n",
      "Source: huggingface_doc, Chunk ID: 401\n",
      "\n",
      "Result 16:\n",
      "--\n",
      "title: \"Welcome fastText to the Hugging Face Hub\"\n",
      "thumbnail: /blog/assets/147_fasttext/thumbnail.png\n",
      "authors:\n",
      "- user: sheonhan\n",
      "- user: juanpino\n",
      "  guest: true\n",
      "---\n",
      "\n",
      "# Welcome fastText to the Hugging ...\n",
      "Source: huggingface_doc, Chunk ID: 1397\n",
      "\n",
      "Result 17:\n",
      "--\n",
      "title: \"Assisted Generation: a new direction toward low-latency text generation\"\n",
      "thumbnail: /blog/assets/assisted-generation/thumbnail.png\n",
      "authors:\n",
      "- user: joaogante\n",
      "---\n",
      "\n",
      "# Assisted Generation: a n...\n",
      "Source: huggingface_doc, Chunk ID: 2108\n",
      "\n",
      "Result 18:\n",
      "--\n",
      "title: \"NystrÃ¶mformer: Approximating self-attention in linear time and memory via the NystrÃ¶m method\"\n",
      "thumbnail: /blog/assets/86_nystromformer/thumbnail.png\n",
      "authors:\n",
      "- user: asi\n",
      "  guest: true\n",
      "---\n",
      "\n",
      "...\n",
      "Source: huggingface_doc, Chunk ID: 1699\n",
      "\n",
      "Result 19:\n",
      "--\n",
      "title:  Deploy Embedding Models with Hugging Face Inference Endpoints\n",
      "thumbnail: /blog/assets/168_inference_endpoints_embeddings/thumbnail.jpg\n",
      "authors:\n",
      "- user: philschmid\n",
      "---\n",
      "\n",
      "# Deploy Embedding Mo...\n",
      "Source: huggingface_doc, Chunk ID: 2464\n"
     ]
    }
   ],
   "source": [
    "## function for hyde search:\n",
    "def hyde_gen_doc(query, model, tokenizer):\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": query}],\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs, max_new_tokens=300, temperature=0.2, top_p=0.9, do_sample=True\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def hyde_search(query, model, tokenizer, retriever):\n",
    "    hdoc = hyde_gen_doc(query, model, tokenizer)\n",
    "    results = retriever.invoke(hdoc)\n",
    "    return results\n",
    "\n",
    "\n",
    "retrieved_docs = hyde_search(prompt, model, tokenizer, retriever)\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"{doc.page_content[:200]}...\")\n",
    "    print(f\"Source: {doc.metadata['source']}, Chunk ID: {doc.metadata['chunk_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fe962d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
